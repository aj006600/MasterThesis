{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(87)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense ,Dropout \n",
    "from keras.layers import BatchNormalization, GaussianNoise, Activation, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>financing</th>\n",
       "      <th>fi</th>\n",
       "      <th>ii</th>\n",
       "      <th>di</th>\n",
       "      <th>rp</th>\n",
       "      <th>capital</th>\n",
       "      <th>EMA9</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>y_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>530.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>39490.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>12463.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>2342.0</td>\n",
       "      <td>6.0443</td>\n",
       "      <td>521.295251</td>\n",
       "      <td>518.980386</td>\n",
       "      <td>513.251221</td>\n",
       "      <td>5.729165</td>\n",
       "      <td>3.933239</td>\n",
       "      <td>84.477581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>536.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>34839.0</td>\n",
       "      <td>-355.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>-451.0</td>\n",
       "      <td>-1374.0</td>\n",
       "      <td>5.3592</td>\n",
       "      <td>525.437881</td>\n",
       "      <td>522.532126</td>\n",
       "      <td>515.535238</td>\n",
       "      <td>6.996887</td>\n",
       "      <td>4.619674</td>\n",
       "      <td>88.417310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>555.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>55614.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-4163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>530.151835</td>\n",
       "      <td>526.614084</td>\n",
       "      <td>518.179719</td>\n",
       "      <td>8.434365</td>\n",
       "      <td>5.454306</td>\n",
       "      <td>91.005801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>554.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>53393.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>-402.0</td>\n",
       "      <td>8.7664</td>\n",
       "      <td>537.123278</td>\n",
       "      <td>532.531850</td>\n",
       "      <td>521.861371</td>\n",
       "      <td>10.670478</td>\n",
       "      <td>6.574521</td>\n",
       "      <td>93.325963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>580.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>62957.0</td>\n",
       "      <td>-502.0</td>\n",
       "      <td>3278.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>-5041.0</td>\n",
       "      <td>9.0658</td>\n",
       "      <td>545.700404</td>\n",
       "      <td>539.847445</td>\n",
       "      <td>526.412277</td>\n",
       "      <td>13.435169</td>\n",
       "      <td>8.026473</td>\n",
       "      <td>94.939847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>579.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>46556.0</td>\n",
       "      <td>-525.0</td>\n",
       "      <td>16064.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>15223.0</td>\n",
       "      <td>9.1131</td>\n",
       "      <td>554.525921</td>\n",
       "      <td>552.006718</td>\n",
       "      <td>546.507936</td>\n",
       "      <td>5.498782</td>\n",
       "      <td>2.554078</td>\n",
       "      <td>64.654070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>576.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>25367.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>10421.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>6574.0</td>\n",
       "      <td>5.2297</td>\n",
       "      <td>558.020737</td>\n",
       "      <td>555.082608</td>\n",
       "      <td>548.396237</td>\n",
       "      <td>6.686370</td>\n",
       "      <td>3.380536</td>\n",
       "      <td>68.577468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>583.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>53122.0</td>\n",
       "      <td>-749.0</td>\n",
       "      <td>26002.0</td>\n",
       "      <td>-684.0</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>16167.0</td>\n",
       "      <td>7.9897</td>\n",
       "      <td>562.616589</td>\n",
       "      <td>559.069899</td>\n",
       "      <td>550.811331</td>\n",
       "      <td>8.258568</td>\n",
       "      <td>4.356142</td>\n",
       "      <td>78.365103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>581.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>31059.0</td>\n",
       "      <td>-377.0</td>\n",
       "      <td>12982.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>9102.0</td>\n",
       "      <td>6.1995</td>\n",
       "      <td>566.693271</td>\n",
       "      <td>562.751453</td>\n",
       "      <td>553.195677</td>\n",
       "      <td>9.555776</td>\n",
       "      <td>5.396069</td>\n",
       "      <td>84.738876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>579.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>22878.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5044.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2930.0</td>\n",
       "      <td>4.4464</td>\n",
       "      <td>569.354617</td>\n",
       "      <td>565.405076</td>\n",
       "      <td>555.181182</td>\n",
       "      <td>10.223893</td>\n",
       "      <td>6.361634</td>\n",
       "      <td>89.645039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   open   high    low  close   volume  financing       fi   \n",
       "0    2021-01-04  530.0  540.0  528.0  536.0  39490.0      454.0  12463.0  \\\n",
       "1    2021-01-05  536.0  542.0  535.0  542.0  34839.0     -355.0   2884.0   \n",
       "2    2021-01-06  555.0  555.0  541.0  549.0  55614.0     -256.0   5355.0   \n",
       "3    2021-01-07  554.0  570.0  553.0  565.0  53393.0     2200.0   1671.0   \n",
       "4    2021-01-08  580.0  580.0  571.0  580.0  62957.0     -502.0   3278.0   \n",
       "..          ...    ...    ...    ...    ...      ...        ...      ...   \n",
       "694  2023-11-13  579.0  580.0  571.0  571.0  46556.0     -525.0  16064.0   \n",
       "695  2023-11-14  576.0  576.0  571.0  572.0  25367.0      128.0  10421.0   \n",
       "696  2023-11-15  583.0  583.0  575.0  581.0  53122.0     -749.0  26002.0   \n",
       "697  2023-11-16  581.0  583.0  578.0  583.0  31059.0     -377.0  12982.0   \n",
       "698  2023-11-17  579.0  583.0  579.0  580.0  22878.0       22.0   5044.0   \n",
       "\n",
       "        ii      di       rp  capital        EMA9       EMA12       EMA26   \n",
       "0    -33.0   865.0   2342.0   6.0443  521.295251  518.980386  513.251221  \\\n",
       "1    179.0  -451.0  -1374.0   5.3592  525.437881  522.532126  515.535238   \n",
       "2    105.0 -4163.0      1.0   6.9696  530.151835  526.614084  518.179719   \n",
       "3    -75.0  2060.0   -402.0   8.7664  537.123278  532.531850  521.861371   \n",
       "4    187.0  1176.0  -5041.0   9.0658  545.700404  539.847445  526.412277   \n",
       "..     ...     ...      ...      ...         ...         ...         ...   \n",
       "694  813.0   109.0  15223.0   9.1131  554.525921  552.006718  546.507936   \n",
       "695  -25.0  1044.0   6574.0   5.2297  558.020737  555.082608  548.396237   \n",
       "696 -684.0   -71.0  16167.0   7.9897  562.616589  559.069899  550.811331   \n",
       "697  105.0   518.0   9102.0   6.1995  566.693271  562.751453  553.195677   \n",
       "698  296.0   134.0   2930.0   4.4464  569.354617  565.405076  555.181182   \n",
       "\n",
       "          MACD    Signal      RSI14  y_10  \n",
       "0     5.729165  3.933239  84.477581     1  \n",
       "1     6.996887  4.619674  88.417310     1  \n",
       "2     8.434365  5.454306  91.005801     1  \n",
       "3    10.670478  6.574521  93.325963     1  \n",
       "4    13.435169  8.026473  94.939847     1  \n",
       "..         ...       ...        ...   ...  \n",
       "694   5.498782  2.554078  64.654070     1  \n",
       "695   6.686370  3.380536  68.577468     1  \n",
       "696   8.258568  4.356142  78.365103     0  \n",
       "697   9.555776  5.396069  84.738876     0  \n",
       "698  10.223893  6.361634  89.645039     0  \n",
       "\n",
       "[699 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "TICKER = 2330\n",
    "TP = 10\n",
    "#############\n",
    "\n",
    "# import data #\n",
    "data = pd.read_csv('/Users/yitsung/Desktop/MasterThesis/data/TaiwanStockData_Top100_EMA')\n",
    "ticker_data = data[data['ticker']==TICKER].reset_index(drop=True)\n",
    "ticker_data = ticker_data.drop(columns=['ticker'])\n",
    "\n",
    "# (SMA-P/P, 2class) #\n",
    "ticker_data[f'y_{TP}'] = ticker_data['close'].rolling(window=TP).mean()\n",
    "ticker_data[f'y_{TP}'] = ticker_data[f'y_{TP}'].shift(-TP)\n",
    "ticker_data = ticker_data.dropna().reindex()\n",
    "ticker_data[f'y_{TP}'] = ((ticker_data[f'y_{TP}'] - ticker_data['close']) >= 0).astype(int)\n",
    "\n",
    "ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.Splite data into train(Library) and test(Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Library = ticker_data[ticker_data['Date'] <= '2023-06-30'] # windows=20, the last prediction from Library is 6/30\n",
    "Prediction = ticker_data[(ticker_data['Date'] >= '2023-06-01')&(ticker_data['Date'] <= '2023-10-31')] # windows=20, start from using 6/1 to predict 7/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.Data Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_minmax(Library, Prediction):\n",
    "\n",
    "    # MinMax #\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_to_standardize = Library.columns.to_list()[1 : ] # exclude 'Date'\n",
    "    Library[feature_to_standardize] = scaler.fit_transform(Library[feature_to_standardize])\n",
    "    Prediction[feature_to_standardize] = scaler.fit_transform(Prediction[feature_to_standardize])\n",
    "\n",
    "    return Library, Prediction\n",
    "\n",
    "### splite train set and validation set ###\n",
    "train_Library = Library[: int((len(Library) * 0.8))]\n",
    "valid_Library = Library[int((len(Library) * 0.8)): ]\n",
    "train_Library, valid_Library = make_data_minmax(Library=train_Library, Prediction=valid_Library)\n",
    "\n",
    "### splite whole data ###\n",
    "Library, Prediction = make_data_minmax(Library=Library, Prediction=Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.Make window data: X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data, window_size):\n",
    "\n",
    "    X = np.array(data.iloc[:, 1: -1])\n",
    "    y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "    data_X, data_y = [], []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        data_X.append(X[i : (i + window_size), :])\n",
    "        data_y.append(y[i + window_size - 1])\n",
    "\n",
    "    data_X, data_y = np.array(data_X), np.array(data_y)\n",
    "        \n",
    "    return data_X, data_y\n",
    "\n",
    "### train set and validation set ###\n",
    "train_X, train_y = data_preprocess(data=train_Library, window_size=20)\n",
    "valid_X, valid_y = data_preprocess(data=valid_Library, window_size=20)\n",
    "\n",
    "### whole data ###\n",
    "# full_X, full_y = data_preprocess(data=Library, window_size=20) # just test \n",
    "test_X, test_y = data_preprocess(data=Prediction, window_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.5.Flatten(MLP only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_flatten(X):\n",
    "    X_flatten = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "\n",
    "    return X_flatten\n",
    "\n",
    "### train set and validation set ###\n",
    "train_X = make_X_flatten(train_X)\n",
    "valid_X = make_X_flatten(valid_X)\n",
    "\n",
    "### whole data ###\n",
    "# full_X = make_X_flatten(full_X) # just test \n",
    "test_X = make_X_flatten(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5.Over-smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resampled train_X: (514, 340)\n",
      "Shape of resampled train_y: (514, 1)\n",
      "Number of positive samples after resampling: 257.0\n",
      "Shape of resampled valid_X: (104, 340)\n",
      "Shape of resampled valid_y: (104, 1)\n",
      "Number of positive samples after resampling: 52.0\n"
     ]
    }
   ],
   "source": [
    "### train set ###\n",
    "ros = RandomOverSampler(random_state=87)\n",
    "train_X_resampled, train_y_resampled = ros.fit_resample(train_X, train_y)\n",
    "train_y_resampled = train_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "print(\"Shape of resampled train_X:\", train_X_resampled.shape)\n",
    "print(\"Shape of resampled train_y:\", train_y_resampled.shape)\n",
    "print(\"Number of positive samples after resampling:\", train_y_resampled.sum())\n",
    "\n",
    "### validation set ###\n",
    "ros = RandomOverSampler(random_state=87)\n",
    "valid_X_resampled, valid_y_resampled = ros.fit_resample(valid_X, valid_y)\n",
    "valid_y_resampled = valid_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "print(\"Shape of resampled valid_X:\", valid_X_resampled.shape)\n",
    "print(\"Shape of resampled valid_y:\", valid_y_resampled.shape)\n",
    "print(\"Number of positive samples after resampling:\", valid_y_resampled.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "TUNNING = True\n",
    "params = {'X_shape': train_X.shape,\n",
    "          'hidden_units': [112, 48, 48, 112, 64], \n",
    "          'dropout_rates': [0.0, 0.8, 0.8, 0.8, 0.0, 0.5, 0.2],\n",
    "          'ls': 0.01, 'lr': 0.01}\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.Create model and find hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 28s]\n",
      "val_action_AUC: 0.6742787957191467\n",
      "\n",
      "Best val_action_AUC So Far: 0.6742787957191467\n",
      "Total elapsed time: 00h 09m 54s\n",
      "Best Hyperparameters:\n",
      "{'units_1': 32, 'units_2': 80, 'units_3': 16, 'units_4': 112, 'units_5': 128, 'dropout_1': 0.2, 'dropout_2': 0.5, 'dropout_3': 0.5, 'dropout_4': 0.0, 'dropout_5': 0.8, 'dropout_6': 0.2, 'dropout_7': 0.0, 'ls': 1e-05, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "def tunning_model(hp, X_shape):\n",
    "\n",
    "    tf.random.set_seed(87)\n",
    "\n",
    "    #############################################\n",
    "    hidden_units = [hp.Int(name=f\"units_{i}\", min_value=16, max_value=128, step=16) for i in range(1, 6)]\n",
    "    dropout_rates = [hp.Choice(f\"dropout_{i}\", [0.0, 0.2, 0.5, 0.8]) for i in range(1, 8)]\n",
    "    ls = hp.Choice('ls',[1e-2, 1e-3, 1e-5])\n",
    "    lr = hp.Choice('lr',[1e-2, 1e-3, 1e-5])\n",
    "    #############################################\n",
    "    \n",
    "    inp = Input(shape = (X_shape[1], ))\n",
    "    x0 = BatchNormalization()(inp)\n",
    "\n",
    "    encoder = GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = Dense(hidden_units[0])(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = Dense(X_shape[1], name = 'decoder')(decoder)  \n",
    "\n",
    "    x_ae = Dense(hidden_units[1])(decoder)\n",
    "    x_ae = BatchNormalization()(x_ae)\n",
    "    x_ae = Activation('swish')(x_ae)\n",
    "    x_ae = Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "    out_ae = Dense(1, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "    \n",
    "    x = Concatenate()([x0, encoder])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[3])(x)\n",
    "\n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = Dense(hidden_units[i])(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    out = Dense(1, activation = 'sigmoid', name = 'action')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=[decoder, out_ae, out])\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n",
    "                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls),\n",
    "                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                         },\n",
    "                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name='MAE'), \n",
    "                             'ae_action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                             'action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                            }, \n",
    "                 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "if TUNNING:\n",
    "    model_fn = lambda hp: tunning_model(hp, X_shape=train_X.shape)\n",
    "    tuner = kt.BayesianOptimization(model_fn,\n",
    "                                    objective=kt.Objective(\"val_action_AUC\", direction=\"max\"),\n",
    "                                    max_trials=10,\n",
    "                                    executions_per_trial=2,\n",
    "                                    directory=\"model_kt\",\n",
    "                                    overwrite=True,\n",
    "                                    seed=87)\n",
    "    path = f'model.hdf5'\n",
    "    ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=10, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    tuner.search(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],\n",
    "                 validation_data=(valid_X_resampled, [valid_X_resampled, valid_y_resampled, valid_y_resampled]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "                 epochs=100, \n",
    "                 batch_size=16, \n",
    "                 callbacks=[ckp, es], \n",
    "                 verbose=1)\n",
    "    \n",
    "    model = tuner.get_best_models()[0]\n",
    "\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.Train model(with parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_shape, hidden_units, dropout_rates, lr, ls):\n",
    "\n",
    "    tf.random.set_seed(87)\n",
    "\n",
    "    inp = Input(shape = (X_shape[1], ))\n",
    "    x0 = BatchNormalization()(inp)\n",
    "\n",
    "    encoder = GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = Dense(hidden_units[0])(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = Dense(X_shape[1], name = 'decoder')(decoder)  \n",
    "\n",
    "    x_ae = Dense(hidden_units[1])(decoder)\n",
    "    x_ae = BatchNormalization()(x_ae)\n",
    "    x_ae = Activation('swish')(x_ae)\n",
    "    x_ae = Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "    out_ae = Dense(1, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "    \n",
    "    x = Concatenate()([x0, encoder])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[3])(x)\n",
    "\n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = Dense(hidden_units[i])(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    out = Dense(1, activation = 'sigmoid', name = 'action')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=[decoder, out_ae, out])\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n",
    "                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls),\n",
    "                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                         },\n",
    "                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name='MAE'), \n",
    "                             'ae_action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                             'action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                            }, \n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "if TUNNING == False:\n",
    "\n",
    "    path = f'model.hdf5'\n",
    "    model = create_model(**params)\n",
    "    ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=10, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    history = model.fit(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],  # full_X_resampled, [full_X_resampled, full_y_resampled, full_y_resampled]\n",
    "                        validation_data=(valid_X_resampled, [valid_X_resampled, valid_y_resampled, valid_y_resampled]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "                        # sample_weight = sw[tr], \n",
    "                        epochs=100, # 100 or coose epochs=n by experience\n",
    "                        batch_size=16, \n",
    "                        callbacks=[ckp, es],                                              # If you want to use, uncomment\n",
    "                        verbose=1)\n",
    "    \n",
    "    tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    score = hist['val_action_AUC'].max()\n",
    "    print(f'AUC:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.Test model on one stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 199ms/step\n",
      "ACC: 0.4642857142857143\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pred  True\n",
       "0      0   1.0\n",
       "1      0   0.0\n",
       "2      0   0.0\n",
       "3      0   0.0\n",
       "4      0   1.0\n",
       "5      0   1.0\n",
       "6      0   1.0\n",
       "7      1   1.0\n",
       "8      1   0.0\n",
       "9      0   0.0\n",
       "10     0   0.0\n",
       "11     0   0.0\n",
       "12     1   0.0\n",
       "13     0   0.0\n",
       "14     0   0.0\n",
       "15     1   1.0\n",
       "16     1   1.0\n",
       "17     1   0.0\n",
       "18     1   0.0\n",
       "19     1   0.0\n",
       "20     1   0.0\n",
       "21     1   0.0\n",
       "22     1   0.0\n",
       "23     1   0.0\n",
       "24     1   0.0\n",
       "25     1   0.0\n",
       "26     1   0.0\n",
       "27     1   0.0\n",
       "28     1   0.0\n",
       "29     1   0.0\n",
       "30     1   1.0\n",
       "31     1   1.0\n",
       "32     1   1.0\n",
       "33     1   1.0\n",
       "34     1   1.0\n",
       "35     1   1.0\n",
       "36     1   1.0\n",
       "37     1   1.0\n",
       "38     1   0.0\n",
       "39     1   1.0\n",
       "40     1   0.0\n",
       "41     1   0.0\n",
       "42     1   0.0\n",
       "43     1   0.0\n",
       "44     1   0.0\n",
       "45     1   0.0\n",
       "46     1   0.0\n",
       "47     1   0.0\n",
       "48     1   0.0\n",
       "49     1   1.0\n",
       "50     1   1.0\n",
       "51     1   0.0\n",
       "52     0   0.0\n",
       "53     1   0.0\n",
       "54     1   0.0\n",
       "55     0   0.0\n",
       "56     0   0.0\n",
       "57     0   0.0\n",
       "58     1   0.0\n",
       "59     1   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = model.predict(test_X) \n",
    "pred_dir = pred_dir[2]\n",
    "pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "result_df['True'] = test_y\n",
    "\n",
    "match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "correct = match_count / len(result_df)\n",
    "\n",
    "print(f'ACC: {correct}\\n')\n",
    "result_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " now: processing 2330 \n",
      "\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 15s 289ms/step - loss: 2.6287 - decoder_loss: 0.8376 - ae_action_loss: 0.9742 - action_loss: 0.8168 - decoder_MAE: 0.6583 - ae_action_AUC: 0.5284 - action_AUC: 0.5487 - val_loss: 1.5680 - val_decoder_loss: 0.1757 - val_ae_action_loss: 0.7026 - val_action_loss: 0.6896 - val_decoder_MAE: 0.3204 - val_ae_action_AUC: 0.5950 - val_action_AUC: 0.6287\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 1.7345 - decoder_loss: 0.3099 - ae_action_loss: 0.7328 - action_loss: 0.6918 - decoder_MAE: 0.3928 - ae_action_AUC: 0.5848 - action_AUC: 0.6089 - val_loss: 1.4322 - val_decoder_loss: 0.0599 - val_ae_action_loss: 0.6888 - val_action_loss: 0.6835 - val_decoder_MAE: 0.1889 - val_ae_action_AUC: 0.5429 - val_action_AUC: 0.6159\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 1.5191 - decoder_loss: 0.1629 - ae_action_loss: 0.6996 - action_loss: 0.6566 - decoder_MAE: 0.2921 - ae_action_AUC: 0.6146 - action_AUC: 0.6559 - val_loss: 1.4469 - val_decoder_loss: 0.0556 - val_ae_action_loss: 0.7054 - val_action_loss: 0.6859 - val_decoder_MAE: 0.1836 - val_ae_action_AUC: 0.6379 - val_action_AUC: 0.5815\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 1.4230 - decoder_loss: 0.1109 - ae_action_loss: 0.6783 - action_loss: 0.6338 - decoder_MAE: 0.2460 - ae_action_AUC: 0.6174 - action_AUC: 0.6955 - val_loss: 1.4359 - val_decoder_loss: 0.0548 - val_ae_action_loss: 0.6874 - val_action_loss: 0.6937 - val_decoder_MAE: 0.1832 - val_ae_action_AUC: 0.5653 - val_action_AUC: 0.5954\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 1.4264 - decoder_loss: 0.0967 - ae_action_loss: 0.6899 - action_loss: 0.6398 - decoder_MAE: 0.2297 - ae_action_AUC: 0.6271 - action_AUC: 0.6901 - val_loss: 1.4374 - val_decoder_loss: 0.0559 - val_ae_action_loss: 0.6822 - val_action_loss: 0.6993 - val_decoder_MAE: 0.1858 - val_ae_action_AUC: 0.6287 - val_action_AUC: 0.6204\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 1.3905 - decoder_loss: 0.0881 - ae_action_loss: 0.6644 - action_loss: 0.6379 - decoder_MAE: 0.2201 - ae_action_AUC: 0.6666 - action_AUC: 0.7084 - val_loss: 1.4502 - val_decoder_loss: 0.0544 - val_ae_action_loss: 0.6800 - val_action_loss: 0.7158 - val_decoder_MAE: 0.1847 - val_ae_action_AUC: 0.6372 - val_action_AUC: 0.6213\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.3046 - decoder_loss: 0.0806 - ae_action_loss: 0.6317 - action_loss: 0.5922 - decoder_MAE: 0.2117 - ae_action_AUC: 0.7178 - action_AUC: 0.7540 - val_loss: 1.5084 - val_decoder_loss: 0.0563 - val_ae_action_loss: 0.6833 - val_action_loss: 0.7688 - val_decoder_MAE: 0.1886 - val_ae_action_AUC: 0.6550 - val_action_AUC: 0.5312\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.3144 - decoder_loss: 0.0771 - ae_action_loss: 0.6654 - action_loss: 0.5719 - decoder_MAE: 0.2089 - ae_action_AUC: 0.6767 - action_AUC: 0.7749 - val_loss: 1.4733 - val_decoder_loss: 0.0551 - val_ae_action_loss: 0.6802 - val_action_loss: 0.7380 - val_decoder_MAE: 0.1844 - val_ae_action_AUC: 0.6411 - val_action_AUC: 0.6444\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.2445 - decoder_loss: 0.0719 - ae_action_loss: 0.6066 - action_loss: 0.5660 - decoder_MAE: 0.2001 - ae_action_AUC: 0.7342 - action_AUC: 0.7909 - val_loss: 1.6112 - val_decoder_loss: 0.0530 - val_ae_action_loss: 0.6831 - val_action_loss: 0.8750 - val_decoder_MAE: 0.1821 - val_ae_action_AUC: 0.6182 - val_action_AUC: 0.5762\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 1.2235 - decoder_loss: 0.0718 - ae_action_loss: 0.5985 - action_loss: 0.5532 - decoder_MAE: 0.1982 - ae_action_AUC: 0.7458 - action_AUC: 0.7957 - val_loss: 1.8281 - val_decoder_loss: 0.0586 - val_ae_action_loss: 0.7085 - val_action_loss: 1.0611 - val_decoder_MAE: 0.1879 - val_ae_action_AUC: 0.5904 - val_action_AUC: 0.5516\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.2152 - decoder_loss: 0.0782 - ae_action_loss: 0.6100 - action_loss: 0.5270 - decoder_MAE: 0.2102 - ae_action_AUC: 0.7382 - action_AUC: 0.8181 - val_loss: 1.8375 - val_decoder_loss: 0.0553 - val_ae_action_loss: 0.6813 - val_action_loss: 1.1009 - val_decoder_MAE: 0.1837 - val_ae_action_AUC: 0.6463 - val_action_AUC: 0.5712\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.1754 - decoder_loss: 0.0652 - ae_action_loss: 0.5809 - action_loss: 0.5294 - decoder_MAE: 0.1938 - ae_action_AUC: 0.7785 - action_AUC: 0.8157 - val_loss: 1.8527 - val_decoder_loss: 0.0595 - val_ae_action_loss: 0.7091 - val_action_loss: 1.0841 - val_decoder_MAE: 0.1920 - val_ae_action_AUC: 0.5717 - val_action_AUC: 0.5425\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.1840 - decoder_loss: 0.0749 - ae_action_loss: 0.5803 - action_loss: 0.5288 - decoder_MAE: 0.2054 - ae_action_AUC: 0.7763 - action_AUC: 0.8262 - val_loss: 1.6820 - val_decoder_loss: 0.0605 - val_ae_action_loss: 0.7051 - val_action_loss: 0.9164 - val_decoder_MAE: 0.1943 - val_ae_action_AUC: 0.6032 - val_action_AUC: 0.4817\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 1.1398 - decoder_loss: 0.0681 - ae_action_loss: 0.5740 - action_loss: 0.4978 - decoder_MAE: 0.1948 - ae_action_AUC: 0.7853 - action_AUC: 0.8425 - val_loss: 2.3555 - val_decoder_loss: 0.0741 - val_ae_action_loss: 0.7654 - val_action_loss: 1.5160 - val_decoder_MAE: 0.2136 - val_ae_action_AUC: 0.4798 - val_action_AUC: 0.4675\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 1.4290 - decoder_loss: 0.2598 - ae_action_loss: 0.6486 - action_loss: 0.5205 - decoder_MAE: 0.3631 - ae_action_AUC: 0.6902 - action_AUC: 0.8241 - val_loss: 2.0225 - val_decoder_loss: 0.2558 - val_ae_action_loss: 0.6846 - val_action_loss: 1.0820 - val_decoder_MAE: 0.4078 - val_ae_action_AUC: 0.6202 - val_action_AUC: 0.4602\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.2071 - decoder_loss: 0.1236 - ae_action_loss: 0.5865 - action_loss: 0.4971 - decoder_MAE: 0.2671 - ae_action_AUC: 0.7766 - action_AUC: 0.8449 - val_loss: 2.5218 - val_decoder_loss: 0.2287 - val_ae_action_loss: 0.8755 - val_action_loss: 1.4176 - val_decoder_MAE: 0.3746 - val_ae_action_AUC: 0.5170 - val_action_AUC: 0.4405\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.1526 - decoder_loss: 0.0963 - ae_action_loss: 0.5615 - action_loss: 0.4948 - decoder_MAE: 0.2339 - ae_action_AUC: 0.7892 - action_AUC: 0.8461 - val_loss: 2.0565 - val_decoder_loss: 0.1277 - val_ae_action_loss: 0.7686 - val_action_loss: 1.1602 - val_decoder_MAE: 0.2796 - val_ae_action_AUC: 0.4541 - val_action_AUC: 0.4891\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - ETA: 0s - loss: 1.1304 - decoder_loss: 0.0965 - ae_action_loss: 0.5427 - action_loss: 0.4912 - decoder_MAE: 0.2343 - ae_action_AUC: 0.8143 - action_AUC: 0.8478Restoring model weights from the end of the best epoch: 8.\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 1.1304 - decoder_loss: 0.0965 - ae_action_loss: 0.5427 - action_loss: 0.4912 - decoder_MAE: 0.2343 - ae_action_AUC: 0.8143 - action_AUC: 0.8478 - val_loss: 2.2485 - val_decoder_loss: 0.0940 - val_ae_action_loss: 0.7167 - val_action_loss: 1.4378 - val_decoder_MAE: 0.2390 - val_ae_action_AUC: 0.4669 - val_action_AUC: 0.4351\n",
      "Epoch 18: early stopping\n",
      "AUC: 0.6444156169891357\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x575a401f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 2s 302ms/step\n",
      "\n",
      " ACC: 0.35714285714285715 \n",
      "\n",
      "\n",
      " now: processing 2454 \n",
      "\n",
      "2454 import data failed.\n",
      "\n",
      " now: processing 2317 \n",
      "\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 12s 193ms/step - loss: 2.5203 - decoder_loss: 0.7684 - ae_action_loss: 0.9455 - action_loss: 0.8063 - decoder_MAE: 0.6219 - ae_action_AUC: 0.4942 - action_AUC: 0.5343 - val_loss: 1.6651 - val_decoder_loss: 0.1678 - val_ae_action_loss: 0.7139 - val_action_loss: 0.7833 - val_decoder_MAE: 0.3126 - val_ae_action_AUC: 0.4728 - val_action_AUC: 0.6126\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.7329 - decoder_loss: 0.2809 - ae_action_loss: 0.7516 - action_loss: 0.7004 - decoder_MAE: 0.3781 - ae_action_AUC: 0.5701 - action_AUC: 0.5712 - val_loss: 1.4923 - val_decoder_loss: 0.0809 - val_ae_action_loss: 0.7071 - val_action_loss: 0.7043 - val_decoder_MAE: 0.2222 - val_ae_action_AUC: 0.6330 - val_action_AUC: 0.6394\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 1.5397 - decoder_loss: 0.1385 - ae_action_loss: 0.7143 - action_loss: 0.6869 - decoder_MAE: 0.2690 - ae_action_AUC: 0.5627 - action_AUC: 0.6021 - val_loss: 1.4484 - val_decoder_loss: 0.0650 - val_ae_action_loss: 0.6913 - val_action_loss: 0.6922 - val_decoder_MAE: 0.1989 - val_ae_action_AUC: 0.5741 - val_action_AUC: 0.6139\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.4612 - decoder_loss: 0.0989 - ae_action_loss: 0.6923 - action_loss: 0.6700 - decoder_MAE: 0.2289 - ae_action_AUC: 0.5999 - action_AUC: 0.6468 - val_loss: 1.4524 - val_decoder_loss: 0.0605 - val_ae_action_loss: 0.6913 - val_action_loss: 0.7006 - val_decoder_MAE: 0.1906 - val_ae_action_AUC: 0.5423 - val_action_AUC: 0.6276\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.4243 - decoder_loss: 0.0851 - ae_action_loss: 0.6739 - action_loss: 0.6652 - decoder_MAE: 0.2102 - ae_action_AUC: 0.6330 - action_AUC: 0.6412 - val_loss: 1.4753 - val_decoder_loss: 0.0621 - val_ae_action_loss: 0.6923 - val_action_loss: 0.7208 - val_decoder_MAE: 0.1924 - val_ae_action_AUC: 0.6381 - val_action_AUC: 0.6289\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.4535 - decoder_loss: 0.0737 - ae_action_loss: 0.6925 - action_loss: 0.6874 - decoder_MAE: 0.1982 - ae_action_AUC: 0.5996 - action_AUC: 0.6069 - val_loss: 1.4496 - val_decoder_loss: 0.0619 - val_ae_action_loss: 0.6931 - val_action_loss: 0.6946 - val_decoder_MAE: 0.1944 - val_ae_action_AUC: 0.6315 - val_action_AUC: 0.6328\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 1.3652 - decoder_loss: 0.0575 - ae_action_loss: 0.6657 - action_loss: 0.6420 - decoder_MAE: 0.1782 - ae_action_AUC: 0.6510 - action_AUC: 0.6931 - val_loss: 1.4601 - val_decoder_loss: 0.0612 - val_ae_action_loss: 0.6885 - val_action_loss: 0.7103 - val_decoder_MAE: 0.1942 - val_ae_action_AUC: 0.6097 - val_action_AUC: 0.6113\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 1.3648 - decoder_loss: 0.0645 - ae_action_loss: 0.6639 - action_loss: 0.6365 - decoder_MAE: 0.1853 - ae_action_AUC: 0.6479 - action_AUC: 0.6905 - val_loss: 1.4585 - val_decoder_loss: 0.0589 - val_ae_action_loss: 0.7035 - val_action_loss: 0.6962 - val_decoder_MAE: 0.1905 - val_ae_action_AUC: 0.6411 - val_action_AUC: 0.5956\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 1.3896 - decoder_loss: 0.0680 - ae_action_loss: 0.6873 - action_loss: 0.6343 - decoder_MAE: 0.1872 - ae_action_AUC: 0.6202 - action_AUC: 0.6805 - val_loss: 1.4417 - val_decoder_loss: 0.0542 - val_ae_action_loss: 0.6894 - val_action_loss: 0.6981 - val_decoder_MAE: 0.1811 - val_ae_action_AUC: 0.6659 - val_action_AUC: 0.6270\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.2974 - decoder_loss: 0.0566 - ae_action_loss: 0.6269 - action_loss: 0.6139 - decoder_MAE: 0.1754 - ae_action_AUC: 0.7191 - action_AUC: 0.7364 - val_loss: 1.4549 - val_decoder_loss: 0.0573 - val_ae_action_loss: 0.6957 - val_action_loss: 0.7020 - val_decoder_MAE: 0.1847 - val_ae_action_AUC: 0.6413 - val_action_AUC: 0.6065\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.2267 - decoder_loss: 0.0489 - ae_action_loss: 0.6003 - action_loss: 0.5774 - decoder_MAE: 0.1628 - ae_action_AUC: 0.7501 - action_AUC: 0.7711 - val_loss: 1.4606 - val_decoder_loss: 0.0635 - val_ae_action_loss: 0.6923 - val_action_loss: 0.7049 - val_decoder_MAE: 0.1949 - val_ae_action_AUC: 0.5828 - val_action_AUC: 0.6050\n",
      "Epoch 12/100\n",
      "30/31 [============================>.] - ETA: 0s - loss: 1.2742 - decoder_loss: 0.0609 - ae_action_loss: 0.6349 - action_loss: 0.5784 - decoder_MAE: 0.1799 - ae_action_AUC: 0.7038 - action_AUC: 0.7718Restoring model weights from the end of the best epoch: 2.\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.2726 - decoder_loss: 0.0607 - ae_action_loss: 0.6331 - action_loss: 0.5788 - decoder_MAE: 0.1799 - ae_action_AUC: 0.7051 - action_AUC: 0.7705 - val_loss: 1.4943 - val_decoder_loss: 0.0581 - val_ae_action_loss: 0.7168 - val_action_loss: 0.7194 - val_decoder_MAE: 0.1870 - val_ae_action_AUC: 0.6289 - val_action_AUC: 0.6250\n",
      "Epoch 12: early stopping\n",
      "AUC: 0.6394230723381042\n",
      "3/3 [==============================] - 2s 190ms/step\n",
      "\n",
      " ACC: 0.5833333333333334 \n",
      "\n",
      "\n",
      " now: processing 2308 \n",
      "\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 19s 373ms/step - loss: 2.7409 - decoder_loss: 0.8716 - ae_action_loss: 1.0864 - action_loss: 0.7829 - decoder_MAE: 0.6876 - ae_action_AUC: 0.4730 - action_AUC: 0.5528 - val_loss: 1.6119 - val_decoder_loss: 0.1704 - val_ae_action_loss: 0.7107 - val_action_loss: 0.7309 - val_decoder_MAE: 0.3216 - val_ae_action_AUC: 0.5882 - val_action_AUC: 0.5851\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.7900 - decoder_loss: 0.2994 - ae_action_loss: 0.7707 - action_loss: 0.7199 - decoder_MAE: 0.4017 - ae_action_AUC: 0.5385 - action_AUC: 0.5605 - val_loss: 1.4620 - val_decoder_loss: 0.0723 - val_ae_action_loss: 0.6921 - val_action_loss: 0.6976 - val_decoder_MAE: 0.2097 - val_ae_action_AUC: 0.6069 - val_action_AUC: 0.3902\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.5592 - decoder_loss: 0.1718 - ae_action_loss: 0.7059 - action_loss: 0.6815 - decoder_MAE: 0.3010 - ae_action_AUC: 0.5643 - action_AUC: 0.5867 - val_loss: 1.4508 - val_decoder_loss: 0.0642 - val_ae_action_loss: 0.6950 - val_action_loss: 0.6916 - val_decoder_MAE: 0.2001 - val_ae_action_AUC: 0.4384 - val_action_AUC: 0.5963\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.4828 - decoder_loss: 0.1132 - ae_action_loss: 0.6919 - action_loss: 0.6778 - decoder_MAE: 0.2476 - ae_action_AUC: 0.5794 - action_AUC: 0.5906 - val_loss: 1.4656 - val_decoder_loss: 0.0645 - val_ae_action_loss: 0.7022 - val_action_loss: 0.6990 - val_decoder_MAE: 0.2022 - val_ae_action_AUC: 0.5708 - val_action_AUC: 0.5645\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.4831 - decoder_loss: 0.0869 - ae_action_loss: 0.6959 - action_loss: 0.7003 - decoder_MAE: 0.2201 - ae_action_AUC: 0.5657 - action_AUC: 0.5646 - val_loss: 1.4584 - val_decoder_loss: 0.0626 - val_ae_action_loss: 0.6997 - val_action_loss: 0.6961 - val_decoder_MAE: 0.1985 - val_ae_action_AUC: 0.5795 - val_action_AUC: 0.5677\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.4463 - decoder_loss: 0.0724 - ae_action_loss: 0.6950 - action_loss: 0.6789 - decoder_MAE: 0.2002 - ae_action_AUC: 0.5811 - action_AUC: 0.6051 - val_loss: 1.4689 - val_decoder_loss: 0.0622 - val_ae_action_loss: 0.7115 - val_action_loss: 0.6952 - val_decoder_MAE: 0.1980 - val_ae_action_AUC: 0.5640 - val_action_AUC: 0.5932\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.4314 - decoder_loss: 0.0668 - ae_action_loss: 0.6887 - action_loss: 0.6759 - decoder_MAE: 0.1926 - ae_action_AUC: 0.5936 - action_AUC: 0.5998 - val_loss: 1.4586 - val_decoder_loss: 0.0607 - val_ae_action_loss: 0.7006 - val_action_loss: 0.6973 - val_decoder_MAE: 0.1939 - val_ae_action_AUC: 0.4625 - val_action_AUC: 0.5427\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.5784 - decoder_loss: 0.2077 - ae_action_loss: 0.7030 - action_loss: 0.6677 - decoder_MAE: 0.3044 - ae_action_AUC: 0.5261 - action_AUC: 0.6120 - val_loss: 1.4607 - val_decoder_loss: 0.0654 - val_ae_action_loss: 0.6936 - val_action_loss: 0.7016 - val_decoder_MAE: 0.2008 - val_ae_action_AUC: 0.5658 - val_action_AUC: 0.6054\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.4476 - decoder_loss: 0.0950 - ae_action_loss: 0.6949 - action_loss: 0.6577 - decoder_MAE: 0.2244 - ae_action_AUC: 0.5772 - action_AUC: 0.6602 - val_loss: 1.4714 - val_decoder_loss: 0.0608 - val_ae_action_loss: 0.6960 - val_action_loss: 0.7146 - val_decoder_MAE: 0.1930 - val_ae_action_AUC: 0.4029 - val_action_AUC: 0.4229\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.4464 - decoder_loss: 0.0719 - ae_action_loss: 0.6954 - action_loss: 0.6791 - decoder_MAE: 0.2024 - ae_action_AUC: 0.5507 - action_AUC: 0.6109 - val_loss: 1.4648 - val_decoder_loss: 0.0608 - val_ae_action_loss: 0.6947 - val_action_loss: 0.7093 - val_decoder_MAE: 0.1945 - val_ae_action_AUC: 0.5588 - val_action_AUC: 0.6211\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.3959 - decoder_loss: 0.0607 - ae_action_loss: 0.6706 - action_loss: 0.6646 - decoder_MAE: 0.1854 - ae_action_AUC: 0.6158 - action_AUC: 0.6052 - val_loss: 1.4843 - val_decoder_loss: 0.0661 - val_ae_action_loss: 0.6997 - val_action_loss: 0.7185 - val_decoder_MAE: 0.2010 - val_ae_action_AUC: 0.4055 - val_action_AUC: 0.4336\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.3907 - decoder_loss: 0.0628 - ae_action_loss: 0.6803 - action_loss: 0.6476 - decoder_MAE: 0.1892 - ae_action_AUC: 0.5879 - action_AUC: 0.6730 - val_loss: 1.5028 - val_decoder_loss: 0.0646 - val_ae_action_loss: 0.7011 - val_action_loss: 0.7371 - val_decoder_MAE: 0.1998 - val_ae_action_AUC: 0.5686 - val_action_AUC: 0.4717\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.3279 - decoder_loss: 0.0490 - ae_action_loss: 0.6583 - action_loss: 0.6206 - decoder_MAE: 0.1679 - ae_action_AUC: 0.6590 - action_AUC: 0.7124 - val_loss: 1.5679 - val_decoder_loss: 0.0650 - val_ae_action_loss: 0.7078 - val_action_loss: 0.7950 - val_decoder_MAE: 0.2019 - val_ae_action_AUC: 0.5560 - val_action_AUC: 0.4540\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.3650 - decoder_loss: 0.0663 - ae_action_loss: 0.6663 - action_loss: 0.6323 - decoder_MAE: 0.1915 - ae_action_AUC: 0.6264 - action_AUC: 0.7000 - val_loss: 1.5735 - val_decoder_loss: 0.0691 - val_ae_action_loss: 0.7210 - val_action_loss: 0.7834 - val_decoder_MAE: 0.2068 - val_ae_action_AUC: 0.5006 - val_action_AUC: 0.4551\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.3479 - decoder_loss: 0.0631 - ae_action_loss: 0.6654 - action_loss: 0.6194 - decoder_MAE: 0.1870 - ae_action_AUC: 0.6400 - action_AUC: 0.7155 - val_loss: 1.5968 - val_decoder_loss: 0.0773 - val_ae_action_loss: 0.7126 - val_action_loss: 0.8069 - val_decoder_MAE: 0.2200 - val_ae_action_AUC: 0.5825 - val_action_AUC: 0.4623\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.3064 - decoder_loss: 0.0545 - ae_action_loss: 0.6419 - action_loss: 0.6101 - decoder_MAE: 0.1757 - ae_action_AUC: 0.6762 - action_AUC: 0.7121 - val_loss: 1.6660 - val_decoder_loss: 0.0876 - val_ae_action_loss: 0.7351 - val_action_loss: 0.8433 - val_decoder_MAE: 0.2348 - val_ae_action_AUC: 0.5457 - val_action_AUC: 0.4499\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.3348 - decoder_loss: 0.1123 - ae_action_loss: 0.6429 - action_loss: 0.5796 - decoder_MAE: 0.2286 - ae_action_AUC: 0.6543 - action_AUC: 0.7558 - val_loss: 1.7407 - val_decoder_loss: 0.1044 - val_ae_action_loss: 0.7771 - val_action_loss: 0.8592 - val_decoder_MAE: 0.2563 - val_ae_action_AUC: 0.6115 - val_action_AUC: 0.4466\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.3288 - decoder_loss: 0.0863 - ae_action_loss: 0.6484 - action_loss: 0.5941 - decoder_MAE: 0.2094 - ae_action_AUC: 0.6624 - action_AUC: 0.7443 - val_loss: 1.7352 - val_decoder_loss: 0.1189 - val_ae_action_loss: 0.7734 - val_action_loss: 0.8429 - val_decoder_MAE: 0.2756 - val_ae_action_AUC: 0.6444 - val_action_AUC: 0.4713\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.3375 - decoder_loss: 0.1089 - ae_action_loss: 0.6271 - action_loss: 0.6014 - decoder_MAE: 0.2304 - ae_action_AUC: 0.7008 - action_AUC: 0.7334 - val_loss: 1.7599 - val_decoder_loss: 0.1347 - val_ae_action_loss: 0.7930 - val_action_loss: 0.8323 - val_decoder_MAE: 0.2929 - val_ae_action_AUC: 0.6154 - val_action_AUC: 0.4893\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3030 - decoder_loss: 0.0964 - ae_action_loss: 0.6311 - action_loss: 0.5755 - decoder_MAE: 0.2144 - ae_action_AUC: 0.6979 - action_AUC: 0.7796Restoring model weights from the end of the best epoch: 10.\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.3030 - decoder_loss: 0.0964 - ae_action_loss: 0.6311 - action_loss: 0.5755 - decoder_MAE: 0.2144 - ae_action_AUC: 0.6979 - action_AUC: 0.7796 - val_loss: 2.1253 - val_decoder_loss: 0.1860 - val_ae_action_loss: 0.9684 - val_action_loss: 0.9710 - val_decoder_MAE: 0.3435 - val_ae_action_AUC: 0.6668 - val_action_AUC: 0.4893\n",
      "Epoch 20: early stopping\n",
      "AUC: 0.6211168766021729\n",
      "3/3 [==============================] - 2s 286ms/step\n",
      "\n",
      " ACC: 0.7261904761904762 \n",
      "\n",
      "\n",
      " now: processing 2382 \n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 13s 237ms/step - loss: 2.5826 - decoder_loss: 0.8116 - ae_action_loss: 0.9780 - action_loss: 0.7930 - decoder_MAE: 0.6749 - ae_action_AUC: 0.5310 - action_AUC: 0.5629 - val_loss: 1.6628 - val_decoder_loss: 0.2256 - val_ae_action_loss: 0.7271 - val_action_loss: 0.7101 - val_decoder_MAE: 0.3582 - val_ae_action_AUC: 0.4834 - val_action_AUC: 0.4693\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 1s 40ms/step - loss: 1.7711 - decoder_loss: 0.2994 - ae_action_loss: 0.7522 - action_loss: 0.7195 - decoder_MAE: 0.4110 - ae_action_AUC: 0.5496 - action_AUC: 0.5552 - val_loss: 1.4889 - val_decoder_loss: 0.0685 - val_ae_action_loss: 0.7192 - val_action_loss: 0.7012 - val_decoder_MAE: 0.2034 - val_ae_action_AUC: 0.5649 - val_action_AUC: 0.4066\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 1.5367 - decoder_loss: 0.1482 - ae_action_loss: 0.7038 - action_loss: 0.6847 - decoder_MAE: 0.2879 - ae_action_AUC: 0.5818 - action_AUC: 0.5909 - val_loss: 1.4567 - val_decoder_loss: 0.0645 - val_ae_action_loss: 0.6996 - val_action_loss: 0.6926 - val_decoder_MAE: 0.1990 - val_ae_action_AUC: 0.5669 - val_action_AUC: 0.5638\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 1.5017 - decoder_loss: 0.1048 - ae_action_loss: 0.7221 - action_loss: 0.6749 - decoder_MAE: 0.2406 - ae_action_AUC: 0.5234 - action_AUC: 0.6291 - val_loss: 1.4672 - val_decoder_loss: 0.0672 - val_ae_action_loss: 0.6954 - val_action_loss: 0.7046 - val_decoder_MAE: 0.2042 - val_ae_action_AUC: 0.4240 - val_action_AUC: 0.3911\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 1.4456 - decoder_loss: 0.0805 - ae_action_loss: 0.6869 - action_loss: 0.6782 - decoder_MAE: 0.2108 - ae_action_AUC: 0.5799 - action_AUC: 0.6084 - val_loss: 1.4722 - val_decoder_loss: 0.0679 - val_ae_action_loss: 0.7014 - val_action_loss: 0.7028 - val_decoder_MAE: 0.2021 - val_ae_action_AUC: 0.5529 - val_action_AUC: 0.5037\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 1.4269 - decoder_loss: 0.0648 - ae_action_loss: 0.6926 - action_loss: 0.6696 - decoder_MAE: 0.1912 - ae_action_AUC: 0.5822 - action_AUC: 0.6295 - val_loss: 1.4926 - val_decoder_loss: 0.0710 - val_ae_action_loss: 0.7004 - val_action_loss: 0.7211 - val_decoder_MAE: 0.2067 - val_ae_action_AUC: 0.5033 - val_action_AUC: 0.4830\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 1.4172 - decoder_loss: 0.0628 - ae_action_loss: 0.6914 - action_loss: 0.6630 - decoder_MAE: 0.1872 - ae_action_AUC: 0.5884 - action_AUC: 0.6610 - val_loss: 1.5266 - val_decoder_loss: 0.0737 - val_ae_action_loss: 0.6961 - val_action_loss: 0.7567 - val_decoder_MAE: 0.2128 - val_ae_action_AUC: 0.4688 - val_action_AUC: 0.3848\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 1.3700 - decoder_loss: 0.0628 - ae_action_loss: 0.6651 - action_loss: 0.6421 - decoder_MAE: 0.1878 - ae_action_AUC: 0.6245 - action_AUC: 0.6846 - val_loss: 1.5337 - val_decoder_loss: 0.0786 - val_ae_action_loss: 0.6948 - val_action_loss: 0.7603 - val_decoder_MAE: 0.2194 - val_ae_action_AUC: 0.5283 - val_action_AUC: 0.4257\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 1.3862 - decoder_loss: 0.0553 - ae_action_loss: 0.6862 - action_loss: 0.6447 - decoder_MAE: 0.1776 - ae_action_AUC: 0.5942 - action_AUC: 0.6803 - val_loss: 1.5263 - val_decoder_loss: 0.0799 - val_ae_action_loss: 0.6993 - val_action_loss: 0.7472 - val_decoder_MAE: 0.2226 - val_ae_action_AUC: 0.5043 - val_action_AUC: 0.4996\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 1.2960 - decoder_loss: 0.0534 - ae_action_loss: 0.6514 - action_loss: 0.5912 - decoder_MAE: 0.1743 - ae_action_AUC: 0.6784 - action_AUC: 0.7582 - val_loss: 1.6048 - val_decoder_loss: 0.0847 - val_ae_action_loss: 0.7192 - val_action_loss: 0.8009 - val_decoder_MAE: 0.2300 - val_ae_action_AUC: 0.5139 - val_action_AUC: 0.5897\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.3020 - decoder_loss: 0.0521 - ae_action_loss: 0.6276 - action_loss: 0.6224 - decoder_MAE: 0.1717 - ae_action_AUC: 0.7053 - action_AUC: 0.7192 - val_loss: 1.7448 - val_decoder_loss: 0.1047 - val_ae_action_loss: 0.7922 - val_action_loss: 0.8478 - val_decoder_MAE: 0.2580 - val_ae_action_AUC: 0.5181 - val_action_AUC: 0.6089\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 1.2621 - decoder_loss: 0.0628 - ae_action_loss: 0.6152 - action_loss: 0.5840 - decoder_MAE: 0.1869 - ae_action_AUC: 0.7372 - action_AUC: 0.7676 - val_loss: 1.7381 - val_decoder_loss: 0.1228 - val_ae_action_loss: 0.8279 - val_action_loss: 0.7874 - val_decoder_MAE: 0.2785 - val_ae_action_AUC: 0.4704 - val_action_AUC: 0.6189\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.2396 - decoder_loss: 0.0631 - ae_action_loss: 0.6067 - action_loss: 0.5698 - decoder_MAE: 0.1891 - ae_action_AUC: 0.7388 - action_AUC: 0.7828 - val_loss: 1.9706 - val_decoder_loss: 0.1191 - val_ae_action_loss: 0.8764 - val_action_loss: 0.9751 - val_decoder_MAE: 0.2744 - val_ae_action_AUC: 0.5629 - val_action_AUC: 0.6335\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 1.2232 - decoder_loss: 0.0680 - ae_action_loss: 0.6158 - action_loss: 0.5394 - decoder_MAE: 0.1948 - ae_action_AUC: 0.7270 - action_AUC: 0.8081 - val_loss: 1.9731 - val_decoder_loss: 0.1189 - val_ae_action_loss: 0.8511 - val_action_loss: 1.0031 - val_decoder_MAE: 0.2748 - val_ae_action_AUC: 0.5076 - val_action_AUC: 0.6487\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.1925 - decoder_loss: 0.0703 - ae_action_loss: 0.5773 - action_loss: 0.5449 - decoder_MAE: 0.1947 - ae_action_AUC: 0.7752 - action_AUC: 0.8018 - val_loss: 1.8958 - val_decoder_loss: 0.1211 - val_ae_action_loss: 0.8219 - val_action_loss: 0.9528 - val_decoder_MAE: 0.2736 - val_ae_action_AUC: 0.5501 - val_action_AUC: 0.6636\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.2348 - decoder_loss: 0.0613 - ae_action_loss: 0.6057 - action_loss: 0.5678 - decoder_MAE: 0.1846 - ae_action_AUC: 0.7556 - action_AUC: 0.7874 - val_loss: 2.1847 - val_decoder_loss: 0.1202 - val_ae_action_loss: 0.8450 - val_action_loss: 1.2194 - val_decoder_MAE: 0.2760 - val_ae_action_AUC: 0.5442 - val_action_AUC: 0.6418\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.1924 - decoder_loss: 0.0652 - ae_action_loss: 0.6062 - action_loss: 0.5209 - decoder_MAE: 0.1870 - ae_action_AUC: 0.7418 - action_AUC: 0.8281 - val_loss: 3.0215 - val_decoder_loss: 0.1299 - val_ae_action_loss: 0.7945 - val_action_loss: 2.0971 - val_decoder_MAE: 0.2868 - val_ae_action_AUC: 0.4835 - val_action_AUC: 0.6230\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.1599 - decoder_loss: 0.0658 - ae_action_loss: 0.5770 - action_loss: 0.5171 - decoder_MAE: 0.1868 - ae_action_AUC: 0.7703 - action_AUC: 0.8271 - val_loss: 2.7327 - val_decoder_loss: 0.1412 - val_ae_action_loss: 0.8109 - val_action_loss: 1.7805 - val_decoder_MAE: 0.2979 - val_ae_action_AUC: 0.5447 - val_action_AUC: 0.6381\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.0939 - decoder_loss: 0.0635 - ae_action_loss: 0.5559 - action_loss: 0.4745 - decoder_MAE: 0.1848 - ae_action_AUC: 0.7990 - action_AUC: 0.8597 - val_loss: 3.1216 - val_decoder_loss: 0.1476 - val_ae_action_loss: 0.8823 - val_action_loss: 2.0917 - val_decoder_MAE: 0.3064 - val_ae_action_AUC: 0.5875 - val_action_AUC: 0.6091\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.0957 - decoder_loss: 0.0639 - ae_action_loss: 0.5510 - action_loss: 0.4808 - decoder_MAE: 0.1844 - ae_action_AUC: 0.7990 - action_AUC: 0.8563 - val_loss: 3.2240 - val_decoder_loss: 0.1805 - val_ae_action_loss: 0.9577 - val_action_loss: 2.0857 - val_decoder_MAE: 0.3404 - val_ae_action_AUC: 0.5884 - val_action_AUC: 0.6311\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.1231 - decoder_loss: 0.0596 - ae_action_loss: 0.5535 - action_loss: 0.5100 - decoder_MAE: 0.1787 - ae_action_AUC: 0.8050 - action_AUC: 0.8356 - val_loss: 3.3341 - val_decoder_loss: 0.2041 - val_ae_action_loss: 0.9025 - val_action_loss: 2.2275 - val_decoder_MAE: 0.3658 - val_ae_action_AUC: 0.5947 - val_action_AUC: 0.6337\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 1.1238 - decoder_loss: 0.0631 - ae_action_loss: 0.5857 - action_loss: 0.4750 - decoder_MAE: 0.1815 - ae_action_AUC: 0.7717 - action_AUC: 0.8571 - val_loss: 2.8494 - val_decoder_loss: 0.1807 - val_ae_action_loss: 0.7206 - val_action_loss: 1.9482 - val_decoder_MAE: 0.3422 - val_ae_action_AUC: 0.5542 - val_action_AUC: 0.6605\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 0.9638 - decoder_loss: 0.0587 - ae_action_loss: 0.5170 - action_loss: 0.3881 - decoder_MAE: 0.1767 - ae_action_AUC: 0.8323 - action_AUC: 0.9102 - val_loss: 4.9402 - val_decoder_loss: 0.1860 - val_ae_action_loss: 0.7623 - val_action_loss: 3.9919 - val_decoder_MAE: 0.3439 - val_ae_action_AUC: 0.5743 - val_action_AUC: 0.5590\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 0.9917 - decoder_loss: 0.0671 - ae_action_loss: 0.5288 - action_loss: 0.3958 - decoder_MAE: 0.1834 - ae_action_AUC: 0.8223 - action_AUC: 0.9055 - val_loss: 7.1400 - val_decoder_loss: 0.3168 - val_ae_action_loss: 0.9168 - val_action_loss: 5.9064 - val_decoder_MAE: 0.4455 - val_ae_action_AUC: 0.6526 - val_action_AUC: 0.5000\n",
      "Epoch 25/100\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.1049 - decoder_loss: 0.0685 - ae_action_loss: 0.5468 - action_loss: 0.4896 - decoder_MAE: 0.1856 - ae_action_AUC: 0.8096 - action_AUC: 0.8553Restoring model weights from the end of the best epoch: 15.\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.1013 - decoder_loss: 0.0677 - ae_action_loss: 0.5466 - action_loss: 0.4870 - decoder_MAE: 0.1847 - ae_action_AUC: 0.8102 - action_AUC: 0.8580 - val_loss: 5.0241 - val_decoder_loss: 0.4540 - val_ae_action_loss: 1.2594 - val_action_loss: 3.3107 - val_decoder_MAE: 0.5293 - val_ae_action_AUC: 0.5930 - val_action_AUC: 0.5447\n",
      "Epoch 25: early stopping\n",
      "AUC: 0.6636464595794678\n",
      "3/3 [==============================] - 2s 181ms/step\n",
      "\n",
      " ACC: 0.5714285714285714 \n",
      "\n",
      "\n",
      " Whole 0050 ACC: 0.5595238095238095 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "TP = 10\n",
    "params = {'X_shape': train_X.shape,\n",
    "          'hidden_units': [112, 48, 48, 112, 64], \n",
    "          'dropout_rates': [0.0, 0.8, 0.8, 0.8, 0.0, 0.5, 0.2],\n",
    "          'ls': 0.01, 'lr': 0.01}\n",
    "# constituent = [2330, 2454, 2317, 2308, 2382, 2303, 2891, 3711, 2881, 2412,\n",
    "#                2886, 2882, 2884, 1216, 2885, 3231, 3034, 2357, 2002, 2892,\n",
    "#                1303, 2379, 5880, 2301, 3037, 2345, 1301, 3008, 3661, 2890,\n",
    "#                5871, 2880, 2327, 2883, 2887, 2207, 4938, 1101, 6669, 1326,\n",
    "#                2395, 3045, 5876, 2603, 1590, 2912, 4904, 2801, 6505, 2408]\n",
    "constituent = [2330, 2454, 2317, 2308, 2382] # just test\n",
    "#############\n",
    "\n",
    "experiment_0050_result = pd.DataFrame()\n",
    "\n",
    "for TICKER in constituent:\n",
    "\n",
    "    print(f'\\n now: processing {TICKER} \\n')\n",
    "\n",
    "    try:\n",
    "        ##### import data #####\n",
    "        data = pd.read_csv('/Users/yitsung/Desktop/MasterThesis/data/TaiwanStockData_Top100_EMA')\n",
    "        ticker_data = data[data['ticker']==TICKER].reset_index(drop=True)\n",
    "        ticker_data = ticker_data.drop(columns=['ticker'])\n",
    "\n",
    "        ticker_data[f'y_{TP}'] = ticker_data['close'].rolling(window=TP).mean()\n",
    "        ticker_data[f'y_{TP}'] = ticker_data[f'y_{TP}'].shift(-TP)\n",
    "        ticker_data = ticker_data.dropna().reindex()\n",
    "        ticker_data[f'y_{TP}'] = ((ticker_data[f'y_{TP}'] - ticker_data['close']) >= 0).astype(int)\n",
    "\n",
    "        ##### Splite data into train(Library) and test(Prediction) #####\n",
    "        Library = ticker_data[ticker_data['Date'] <= '2023-06-30']\n",
    "        Prediction = ticker_data[(ticker_data['Date'] >= '2023-06-01')&(ticker_data['Date'] <= '2023-10-31')]\n",
    "\n",
    "        ##### Data Normalize #####\n",
    "        train_Library = Library[: int((len(Library) * 0.8))]\n",
    "        valid_Library = Library[int((len(Library) * 0.8)): ]\n",
    "\n",
    "        train_Library, valid_Library = make_data_minmax(Library=train_Library, Prediction=valid_Library)\n",
    "        Library, Prediction = make_data_minmax(Library=Library, Prediction=Prediction)\n",
    "\n",
    "    except:\n",
    "        print(f'{TICKER} import data failed.')\n",
    "        continue\n",
    "\n",
    "    ##### Make window data: X, y #####\n",
    "    train_X, train_y = data_preprocess(data=train_Library, window_size=20)\n",
    "    valid_X, valid_y = data_preprocess(data=valid_Library, window_size=20)\n",
    "    test_X, test_y = data_preprocess(data=Prediction, window_size=20)\n",
    "\n",
    "    ##### Flatten(MLP only) #####\n",
    "    train_X = make_X_flatten(train_X)\n",
    "    valid_X = make_X_flatten(valid_X)\n",
    "    test_X = make_X_flatten(test_X)\n",
    "\n",
    "    ##### Over-smapling #####\n",
    "    ros = RandomOverSampler(random_state=87)\n",
    "    train_X_resampled, train_y_resampled = ros.fit_resample(train_X, train_y)\n",
    "    train_y_resampled = train_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "    ##### Train model(with parameter) #####\n",
    "    path = f'model.hdf5'\n",
    "    model = create_model(**params)\n",
    "    ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=10, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    history = model.fit(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],  # full_X_resampled, [full_X_resampled, full_y_resampled, full_y_resampled]\n",
    "                        validation_data=(valid_X_resampled, [valid_X_resampled, valid_y_resampled, valid_y_resampled]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "                        # sample_weight = sw[tr], \n",
    "                        epochs=100, # 100 or coose epochs=n by experience\n",
    "                        batch_size=16, \n",
    "                        callbacks=[ckp, es],                                              # If you want to use, uncomment\n",
    "                        verbose=1)\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    score = hist['val_action_AUC'].max()\n",
    "    print(f'AUC:', score)\n",
    "\n",
    "    ##### Test model on one stock #####\n",
    "    pred_dir = model.predict(test_X) \n",
    "    pred_dir = pred_dir[2]\n",
    "    pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "    result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "    result_df['True'] = test_y\n",
    "\n",
    "    match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "    correct = match_count / len(result_df)\n",
    "    print(f'\\n ACC: {correct} \\n')\n",
    "\n",
    "    tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "    ##### Add to result dataframe #####\n",
    "    experiment_0050_result = pd.concat([experiment_0050_result, result_df], axis=0, ignore_index=True)\n",
    "\n",
    "#### Final ACC ####\n",
    "whole_match_count = (experiment_0050_result['Pred'] == experiment_0050_result['True']).sum()\n",
    "whole_correct = whole_match_count / len(experiment_0050_result)\n",
    "print(f'\\n Whole 0050 ACC: {whole_correct} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Whole 0050 ACC: 0.5833333333333334 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pred  True\n",
       "276     0   1.0\n",
       "277     0   1.0\n",
       "278     0   1.0\n",
       "279     0   1.0\n",
       "280     0   1.0\n",
       "281     0   1.0\n",
       "282     0   1.0\n",
       "283     0   1.0\n",
       "284     0   0.0\n",
       "285     0   0.0\n",
       "286     0   0.0\n",
       "287     0   0.0\n",
       "288     0   0.0\n",
       "289     0   0.0\n",
       "290     0   0.0\n",
       "291     0   1.0\n",
       "292     0   1.0\n",
       "293     0   0.0\n",
       "294     0   0.0\n",
       "295     0   0.0\n",
       "296     0   0.0\n",
       "297     0   0.0\n",
       "298     0   0.0\n",
       "299     0   0.0\n",
       "300     0   0.0\n",
       "301     0   0.0\n",
       "302     0   0.0\n",
       "303     0   1.0\n",
       "304     0   0.0\n",
       "305     0   0.0\n",
       "306     0   0.0\n",
       "307     0   1.0\n",
       "308     0   1.0\n",
       "309     0   1.0\n",
       "310     0   1.0\n",
       "311     0   1.0\n",
       "312     0   1.0\n",
       "313     0   1.0\n",
       "314     0   1.0\n",
       "315     0   1.0\n",
       "316     0   0.0\n",
       "317     0   0.0\n",
       "318     0   0.0\n",
       "319     0   0.0\n",
       "320     0   0.0\n",
       "321     0   0.0\n",
       "322     0   0.0\n",
       "323     0   0.0\n",
       "324     0   0.0\n",
       "325     0   0.0\n",
       "326     0   0.0\n",
       "327     0   0.0\n",
       "328     0   0.0\n",
       "329     0   1.0\n",
       "330     0   0.0\n",
       "331     0   0.0\n",
       "332     0   1.0\n",
       "333     0   1.0\n",
       "334     0   0.0\n",
       "335     0   1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_0050_result.to_csv(f'AE-MLP_Tp={TP}_result.csv', index=False)\n",
    "print(f'\\n Whole 0050 ACC: {whole_correct} \\n')\n",
    "\n",
    "experiment_0050_result.tail(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
