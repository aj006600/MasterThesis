{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(87)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense ,Dropout \n",
    "from keras.layers import BatchNormalization, GaussianNoise, Activation, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>financing</th>\n",
       "      <th>fi</th>\n",
       "      <th>ii</th>\n",
       "      <th>di</th>\n",
       "      <th>rp</th>\n",
       "      <th>capital</th>\n",
       "      <th>EMA9</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>y_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>530.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>39490.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>12463.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>2342.0</td>\n",
       "      <td>6.0443</td>\n",
       "      <td>521.295251</td>\n",
       "      <td>518.980386</td>\n",
       "      <td>513.251221</td>\n",
       "      <td>5.729165</td>\n",
       "      <td>3.933239</td>\n",
       "      <td>84.477581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>536.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>34839.0</td>\n",
       "      <td>-355.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>-451.0</td>\n",
       "      <td>-1374.0</td>\n",
       "      <td>5.3592</td>\n",
       "      <td>525.437881</td>\n",
       "      <td>522.532126</td>\n",
       "      <td>515.535238</td>\n",
       "      <td>6.996887</td>\n",
       "      <td>4.619674</td>\n",
       "      <td>88.417310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>555.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>55614.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-4163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>530.151835</td>\n",
       "      <td>526.614084</td>\n",
       "      <td>518.179719</td>\n",
       "      <td>8.434365</td>\n",
       "      <td>5.454306</td>\n",
       "      <td>91.005801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>554.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>53393.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>-402.0</td>\n",
       "      <td>8.7664</td>\n",
       "      <td>537.123278</td>\n",
       "      <td>532.531850</td>\n",
       "      <td>521.861371</td>\n",
       "      <td>10.670478</td>\n",
       "      <td>6.574521</td>\n",
       "      <td>93.325963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>580.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>62957.0</td>\n",
       "      <td>-502.0</td>\n",
       "      <td>3278.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>-5041.0</td>\n",
       "      <td>9.0658</td>\n",
       "      <td>545.700404</td>\n",
       "      <td>539.847445</td>\n",
       "      <td>526.412277</td>\n",
       "      <td>13.435169</td>\n",
       "      <td>8.026473</td>\n",
       "      <td>94.939847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-11-24</td>\n",
       "      <td>577.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>12503.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>-854.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-118.0</td>\n",
       "      <td>-2263.0</td>\n",
       "      <td>2.8318</td>\n",
       "      <td>575.073961</td>\n",
       "      <td>572.538736</td>\n",
       "      <td>562.541337</td>\n",
       "      <td>9.997398</td>\n",
       "      <td>9.134503</td>\n",
       "      <td>90.744592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>573.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>20322.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-2153.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-3554.0</td>\n",
       "      <td>4.1507</td>\n",
       "      <td>573.659169</td>\n",
       "      <td>571.840469</td>\n",
       "      <td>562.945683</td>\n",
       "      <td>8.894786</td>\n",
       "      <td>9.086560</td>\n",
       "      <td>81.069290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>565.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>26932.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>3323.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>5.1624</td>\n",
       "      <td>573.927335</td>\n",
       "      <td>572.326550</td>\n",
       "      <td>563.838595</td>\n",
       "      <td>8.487955</td>\n",
       "      <td>8.966839</td>\n",
       "      <td>76.500832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>2023-11-29</td>\n",
       "      <td>578.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>27787.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-553.0</td>\n",
       "      <td>-2383.0</td>\n",
       "      <td>4.8624</td>\n",
       "      <td>573.941868</td>\n",
       "      <td>572.584004</td>\n",
       "      <td>564.591292</td>\n",
       "      <td>7.992712</td>\n",
       "      <td>8.772014</td>\n",
       "      <td>71.301362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>576.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>54365.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>4730.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-770.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>7.5527</td>\n",
       "      <td>574.553494</td>\n",
       "      <td>573.263388</td>\n",
       "      <td>565.510455</td>\n",
       "      <td>7.752933</td>\n",
       "      <td>8.568197</td>\n",
       "      <td>68.146342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   open   high    low  close   volume  financing       fi   \n",
       "0    2021-01-04  530.0  540.0  528.0  536.0  39490.0      454.0  12463.0  \\\n",
       "1    2021-01-05  536.0  542.0  535.0  542.0  34839.0     -355.0   2884.0   \n",
       "2    2021-01-06  555.0  555.0  541.0  549.0  55614.0     -256.0   5355.0   \n",
       "3    2021-01-07  554.0  570.0  553.0  565.0  53393.0     2200.0   1671.0   \n",
       "4    2021-01-08  580.0  580.0  571.0  580.0  62957.0     -502.0   3278.0   \n",
       "..          ...    ...    ...    ...    ...      ...        ...      ...   \n",
       "703  2023-11-24  577.0  578.0  574.0  575.0  12503.0      243.0   -854.0   \n",
       "704  2023-11-27  573.0  577.0  568.0  568.0  20322.0     -112.0  -2153.0   \n",
       "705  2023-11-28  565.0  576.0  565.0  575.0  26932.0      478.0   3323.0   \n",
       "706  2023-11-29  578.0  579.0  570.0  574.0  27787.0      357.0   -180.0   \n",
       "707  2023-11-30  576.0  577.0  570.0  577.0  54365.0      -32.0   4730.0   \n",
       "\n",
       "        ii      di      rp  capital        EMA9       EMA12       EMA26   \n",
       "0    -33.0   865.0  2342.0   6.0443  521.295251  518.980386  513.251221  \\\n",
       "1    179.0  -451.0 -1374.0   5.3592  525.437881  522.532126  515.535238   \n",
       "2    105.0 -4163.0     1.0   6.9696  530.151835  526.614084  518.179719   \n",
       "3    -75.0  2060.0  -402.0   8.7664  537.123278  532.531850  521.861371   \n",
       "4    187.0  1176.0 -5041.0   9.0658  545.700404  539.847445  526.412277   \n",
       "..     ...     ...     ...      ...         ...         ...         ...   \n",
       "703   70.0  -118.0 -2263.0   2.8318  575.073961  572.538736  562.541337   \n",
       "704   59.0   -56.0 -3554.0   4.1507  573.659169  571.840469  562.945683   \n",
       "705  -98.0   687.0  -416.0   5.1624  573.927335  572.326550  563.838595   \n",
       "706   55.0  -553.0 -2383.0   4.8624  573.941868  572.584004  564.591292   \n",
       "707  -68.0  -770.0  -155.0   7.5527  574.553494  573.263388  565.510455   \n",
       "\n",
       "          MACD    Signal      RSI14  y_1  \n",
       "0     5.729165  3.933239  84.477581    1  \n",
       "1     6.996887  4.619674  88.417310    1  \n",
       "2     8.434365  5.454306  91.005801    1  \n",
       "3    10.670478  6.574521  93.325963    1  \n",
       "4    13.435169  8.026473  94.939847    1  \n",
       "..         ...       ...        ...  ...  \n",
       "703   9.997398  9.134503  90.744592    0  \n",
       "704   8.894786  9.086560  81.069290    1  \n",
       "705   8.487955  8.966839  76.500832    0  \n",
       "706   7.992712  8.772014  71.301362    1  \n",
       "707   7.752933  8.568197  68.146342    1  \n",
       "\n",
       "[708 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "TICKER = 2330\n",
    "TP = 1\n",
    "#############\n",
    "\n",
    "# import data #\n",
    "data = pd.read_csv('/Users/yitsung/Desktop/MasterThesis/data/TaiwanStockData_Top100_EMA')\n",
    "ticker_data = data[data['ticker']==TICKER].reset_index(drop=True)\n",
    "ticker_data = ticker_data.drop(columns=['ticker'])\n",
    "\n",
    "# (SMA-P/P, 2class) #\n",
    "ticker_data[f'y_{TP}'] = ticker_data['close'].rolling(window=TP).mean()\n",
    "ticker_data[f'y_{TP}'] = ticker_data[f'y_{TP}'].shift(-TP)\n",
    "ticker_data = ticker_data.dropna().reindex()\n",
    "ticker_data[f'y_{TP}'] = ((ticker_data[f'y_{TP}'] - ticker_data['close']) >= 0).astype(int)\n",
    "\n",
    "ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.Splite data into train(Library) and test(Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Library = ticker_data[ticker_data['Date'] <= '2023-06-30'] # windows=20, the last prediction from Library is 6/30\n",
    "Prediction = ticker_data[(ticker_data['Date'] >= '2023-06-01')&(ticker_data['Date'] <= '2023-10-31')] # windows=20, start from using 6/1 to predict 7/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.Data Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_minmax(Library, Prediction):\n",
    "\n",
    "    # MinMax #\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_to_standardize = Library.columns.to_list()[1 : ] # exclude 'Date'\n",
    "    Library[feature_to_standardize] = scaler.fit_transform(Library[feature_to_standardize])\n",
    "    Prediction[feature_to_standardize] = scaler.fit_transform(Prediction[feature_to_standardize])\n",
    "\n",
    "    return Library, Prediction\n",
    "\n",
    "### splite train set and validation set ###\n",
    "train_Library = Library[: int((len(Library) * 0.8))]\n",
    "valid_Library = Library[int((len(Library) * 0.8)): ]\n",
    "train_Library, valid_Library = make_data_minmax(Library=train_Library, Prediction=valid_Library)\n",
    "\n",
    "### splite whole data ###\n",
    "Library, Prediction = make_data_minmax(Library=Library, Prediction=Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.Make window data: X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data, window_size):\n",
    "\n",
    "    X = np.array(data.iloc[:, 1: -1])\n",
    "    y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "    data_X, data_y = [], []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        data_X.append(X[i : (i + window_size), :])\n",
    "        data_y.append(y[i + window_size - 1])\n",
    "\n",
    "    data_X, data_y = np.array(data_X), np.array(data_y)\n",
    "        \n",
    "    return data_X, data_y\n",
    "\n",
    "### train set and validation set ###\n",
    "train_X, train_y = data_preprocess(data=train_Library, window_size=20)\n",
    "valid_X, valid_y = data_preprocess(data=valid_Library, window_size=20)\n",
    "\n",
    "### whole data ###\n",
    "# full_X, full_y = data_preprocess(data=Library, window_size=20) # just test \n",
    "test_X, test_y = data_preprocess(data=Prediction, window_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.5.Flatten(MLP only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_flatten(X):\n",
    "    X_flatten = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "\n",
    "    return X_flatten\n",
    "\n",
    "### train set and validation set ###\n",
    "train_X = make_X_flatten(train_X)\n",
    "valid_X = make_X_flatten(valid_X)\n",
    "\n",
    "### whole data ###\n",
    "# full_X = make_X_flatten(full_X) # just test \n",
    "test_X = make_X_flatten(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5.Over-smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resampled train_X: (478, 340)\n",
      "Shape of resampled train_y: (478, 1)\n",
      "Number of positive samples after resampling: 239.0\n"
     ]
    }
   ],
   "source": [
    "### train set and validation set ###\n",
    "ros = RandomOverSampler(random_state=87)\n",
    "train_X_resampled, train_y_resampled = ros.fit_resample(train_X, train_y)\n",
    "train_y_resampled = train_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "print(\"Shape of resampled train_X:\", train_X_resampled.shape)\n",
    "print(\"Shape of resampled train_y:\", train_y_resampled.shape)\n",
    "print(\"Number of positive samples after resampling:\", train_y_resampled.sum())\n",
    "\n",
    "# ### whole data ###\n",
    "# ros = RandomOverSampler(random_state=87)\n",
    "# full_X_resampled, full_y_resampled = ros.fit_resample(full_X, full_y)\n",
    "# full_y_resampled = full_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "# print(\"Shape of resampled full_X:\", full_X_resampled.shape)\n",
    "# print(\"Shape of resampled full_y:\", full_y_resampled.shape)\n",
    "# print(\"Number of positive samples after resampling:\", full_y_resampled.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "TUNNING = False\n",
    "params = {'X_shape': train_X.shape,\n",
    "          'hidden_units': [32, 64, 16, 32], \n",
    "          'dropout_rates': [0.0, 0.2, 0.8, 0.5, 0.8, 0.2],\n",
    "          'ls': 0.1, 'lr': 0.01}\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.Create model and find hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_model(hp, X_shape):\n",
    "\n",
    "    # tf.random.set_seed(87)\n",
    "\n",
    "    #############################################\n",
    "    hidden_units = [hp.Int(name=f\"units_{i}\", min_value=16, max_value=128, step=16) for i in range(1, 5)]\n",
    "    dropout_rates = [hp.Choice(f\"dropout_{i}\", [0.0, 0.2, 0.5, 0.8]) for i in range(1, 7)]\n",
    "    ls = hp.Choice('ls',[1e-1, 1e-2, 1e-3])\n",
    "    lr = hp.Choice('lr',[1e-1, 1e-2, 1e-3, 1e-4])\n",
    "    #############################################\n",
    "    \n",
    "    inp = Input(shape = (X_shape[1], ))\n",
    "    x0 = BatchNormalization()(inp)\n",
    "\n",
    "    encoder = GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = Dense(hidden_units[0])(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = Dense(X_shape[1], name = 'decoder')(decoder)  \n",
    "\n",
    "    x_ae = Dense(hidden_units[1])(decoder)\n",
    "    x_ae = BatchNormalization()(x_ae)\n",
    "    x_ae = Activation('swish')(x_ae)\n",
    "    x_ae = Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "    out_ae = Dense(1, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "    \n",
    "    x = Concatenate()([x0, encoder])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[3])(x)\n",
    "\n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = Dense(hidden_units[i])(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    out = Dense(1, activation = 'sigmoid', name = 'action')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=[decoder, out_ae, out])\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n",
    "                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls),\n",
    "                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                         },\n",
    "                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name='MAE'), \n",
    "                             'ae_action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                             'action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                            }, \n",
    "                 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "if TUNNING:\n",
    "    model_fn = lambda hp: tunning_model(hp, X_shape=train_X.shape)\n",
    "    tuner = kt.BayesianOptimization(model_fn,\n",
    "                                    objective=kt.Objective(\"val_action_AUC\", direction=\"max\"),\n",
    "                                    max_trials=10,\n",
    "                                    executions_per_trial=1,\n",
    "                                    directory=\"model_kt\",\n",
    "                                    overwrite=True,\n",
    "                                    seed=87)\n",
    "    path = f'model.hdf5'\n",
    "    ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose=0,                      # If you want to use, uncomment\n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=50, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    tuner.search(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],\n",
    "                 validation_data=(valid_X, [valid_X, valid_y, valid_y]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "                 epochs=100,                                                              # 100 or coose epochs=n by experience\n",
    "                #  batch_size=16,\n",
    "                 batch_size=32, \n",
    "                 callbacks=[ckp, es],                                                     # If you want to use, uncomment \n",
    "                 verbose=1)\n",
    "    \n",
    "    model = tuner.get_best_models()[0]\n",
    "\n",
    "    tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.Train model(with parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 14:08:30.016054: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 5s 168ms/step - loss: 1.9244 - decoder_loss: 0.3239 - ae_action_loss: 0.8737 - action_loss: 0.7268 - decoder_MAE: 0.4783 - ae_action_AUC: 0.5370 - action_AUC: 0.5657 - val_loss: 1.7829 - val_decoder_loss: 0.3192 - val_ae_action_loss: 0.7701 - val_action_loss: 0.6936 - val_decoder_MAE: 0.4362 - val_ae_action_AUC: 0.4719 - val_action_AUC: 0.5160\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 1.7014 - decoder_loss: 0.2012 - ae_action_loss: 0.7783 - action_loss: 0.7219 - decoder_MAE: 0.3583 - ae_action_AUC: 0.5465 - action_AUC: 0.5250 - val_loss: 1.5484 - val_decoder_loss: 0.1418 - val_ae_action_loss: 0.7121 - val_action_loss: 0.6945 - val_decoder_MAE: 0.2870 - val_ae_action_AUC: 0.4687 - val_action_AUC: 0.4700\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.5512 - decoder_loss: 0.1030 - ae_action_loss: 0.7378 - action_loss: 0.7104 - decoder_MAE: 0.2517 - ae_action_AUC: 0.5472 - action_AUC: 0.5188 - val_loss: 1.4949 - val_decoder_loss: 0.0801 - val_ae_action_loss: 0.7205 - val_action_loss: 0.6943 - val_decoder_MAE: 0.2217 - val_ae_action_AUC: 0.5056 - val_action_AUC: 0.5004\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.4845 - decoder_loss: 0.0652 - ae_action_loss: 0.7201 - action_loss: 0.6992 - decoder_MAE: 0.1952 - ae_action_AUC: 0.5761 - action_AUC: 0.5346 - val_loss: 1.4676 - val_decoder_loss: 0.0665 - val_ae_action_loss: 0.7058 - val_action_loss: 0.6954 - val_decoder_MAE: 0.2088 - val_ae_action_AUC: 0.5456 - val_action_AUC: 0.4783\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 1.4643 - decoder_loss: 0.0518 - ae_action_loss: 0.7064 - action_loss: 0.7061 - decoder_MAE: 0.1716 - ae_action_AUC: 0.5505 - action_AUC: 0.5336 - val_loss: 1.4469 - val_decoder_loss: 0.0566 - val_ae_action_loss: 0.6972 - val_action_loss: 0.6931 - val_decoder_MAE: 0.1907 - val_ae_action_AUC: 0.4867 - val_action_AUC: 0.5217\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.4763 - decoder_loss: 0.0450 - ae_action_loss: 0.7054 - action_loss: 0.7259 - decoder_MAE: 0.1586 - ae_action_AUC: 0.5547 - action_AUC: 0.4386 - val_loss: 1.4600 - val_decoder_loss: 0.0534 - val_ae_action_loss: 0.7122 - val_action_loss: 0.6945 - val_decoder_MAE: 0.1840 - val_ae_action_AUC: 0.4917 - val_action_AUC: 0.5115\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 1.4154 - decoder_loss: 0.0394 - ae_action_loss: 0.6831 - action_loss: 0.6929 - decoder_MAE: 0.1475 - ae_action_AUC: 0.6103 - action_AUC: 0.5517 - val_loss: 1.4351 - val_decoder_loss: 0.0496 - val_ae_action_loss: 0.6928 - val_action_loss: 0.6927 - val_decoder_MAE: 0.1774 - val_ae_action_AUC: 0.5156 - val_action_AUC: 0.5246\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.4103 - decoder_loss: 0.0368 - ae_action_loss: 0.6793 - action_loss: 0.6941 - decoder_MAE: 0.1421 - ae_action_AUC: 0.6174 - action_AUC: 0.5323 - val_loss: 1.4471 - val_decoder_loss: 0.0484 - val_ae_action_loss: 0.7062 - val_action_loss: 0.6925 - val_decoder_MAE: 0.1748 - val_ae_action_AUC: 0.5012 - val_action_AUC: 0.5056\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.4081 - decoder_loss: 0.0356 - ae_action_loss: 0.6677 - action_loss: 0.7049 - decoder_MAE: 0.1410 - ae_action_AUC: 0.6464 - action_AUC: 0.4899 - val_loss: 1.4393 - val_decoder_loss: 0.0485 - val_ae_action_loss: 0.6979 - val_action_loss: 0.6928 - val_decoder_MAE: 0.1734 - val_ae_action_AUC: 0.5348 - val_action_AUC: 0.4960\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 1.3671 - decoder_loss: 0.0352 - ae_action_loss: 0.6374 - action_loss: 0.6946 - decoder_MAE: 0.1393 - ae_action_AUC: 0.7162 - action_AUC: 0.5436 - val_loss: 1.4358 - val_decoder_loss: 0.0485 - val_ae_action_loss: 0.6955 - val_action_loss: 0.6918 - val_decoder_MAE: 0.1738 - val_ae_action_AUC: 0.5540 - val_action_AUC: 0.5500\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.4060 - decoder_loss: 0.0379 - ae_action_loss: 0.6756 - action_loss: 0.6925 - decoder_MAE: 0.1449 - ae_action_AUC: 0.6726 - action_AUC: 0.5438 - val_loss: 1.4359 - val_decoder_loss: 0.0494 - val_ae_action_loss: 0.6944 - val_action_loss: 0.6921 - val_decoder_MAE: 0.1757 - val_ae_action_AUC: 0.5227 - val_action_AUC: 0.5394\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.3768 - decoder_loss: 0.0332 - ae_action_loss: 0.6410 - action_loss: 0.7026 - decoder_MAE: 0.1356 - ae_action_AUC: 0.6925 - action_AUC: 0.5057 - val_loss: 1.4446 - val_decoder_loss: 0.0494 - val_ae_action_loss: 0.7023 - val_action_loss: 0.6930 - val_decoder_MAE: 0.1757 - val_ae_action_AUC: 0.5608 - val_action_AUC: 0.4992\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.3778 - decoder_loss: 0.0319 - ae_action_loss: 0.6469 - action_loss: 0.6989 - decoder_MAE: 0.1336 - ae_action_AUC: 0.6977 - action_AUC: 0.5037 - val_loss: 1.4297 - val_decoder_loss: 0.0489 - val_ae_action_loss: 0.6875 - val_action_loss: 0.6933 - val_decoder_MAE: 0.1751 - val_ae_action_AUC: 0.5621 - val_action_AUC: 0.5015\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.3388 - decoder_loss: 0.0312 - ae_action_loss: 0.6245 - action_loss: 0.6832 - decoder_MAE: 0.1312 - ae_action_AUC: 0.7262 - action_AUC: 0.5955 - val_loss: 1.4344 - val_decoder_loss: 0.0497 - val_ae_action_loss: 0.6909 - val_action_loss: 0.6938 - val_decoder_MAE: 0.1763 - val_ae_action_AUC: 0.5677 - val_action_AUC: 0.4990\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.3090 - decoder_loss: 0.0289 - ae_action_loss: 0.5915 - action_loss: 0.6887 - decoder_MAE: 0.1265 - ae_action_AUC: 0.7759 - action_AUC: 0.5586 - val_loss: 1.4371 - val_decoder_loss: 0.0492 - val_ae_action_loss: 0.6945 - val_action_loss: 0.6934 - val_decoder_MAE: 0.1748 - val_ae_action_AUC: 0.5388 - val_action_AUC: 0.5265\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.3483 - decoder_loss: 0.0288 - ae_action_loss: 0.6222 - action_loss: 0.6972 - decoder_MAE: 0.1261 - ae_action_AUC: 0.7452 - action_AUC: 0.5274 - val_loss: 1.4354 - val_decoder_loss: 0.0495 - val_ae_action_loss: 0.6927 - val_action_loss: 0.6932 - val_decoder_MAE: 0.1754 - val_ae_action_AUC: 0.5129 - val_action_AUC: 0.5108\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.3255 - decoder_loss: 0.0298 - ae_action_loss: 0.6036 - action_loss: 0.6921 - decoder_MAE: 0.1286 - ae_action_AUC: 0.7686 - action_AUC: 0.5608 - val_loss: 1.4475 - val_decoder_loss: 0.0492 - val_ae_action_loss: 0.7050 - val_action_loss: 0.6933 - val_decoder_MAE: 0.1748 - val_ae_action_AUC: 0.5267 - val_action_AUC: 0.5285\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.2912 - decoder_loss: 0.0297 - ae_action_loss: 0.5671 - action_loss: 0.6945 - decoder_MAE: 0.1288 - ae_action_AUC: 0.8210 - action_AUC: 0.5597 - val_loss: 1.5078 - val_decoder_loss: 0.0505 - val_ae_action_loss: 0.7641 - val_action_loss: 0.6932 - val_decoder_MAE: 0.1763 - val_ae_action_AUC: 0.5052 - val_action_AUC: 0.5138\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 1.3308 - decoder_loss: 0.0307 - ae_action_loss: 0.6189 - action_loss: 0.6813 - decoder_MAE: 0.1293 - ae_action_AUC: 0.7458 - action_AUC: 0.5824 - val_loss: 1.4508 - val_decoder_loss: 0.0484 - val_ae_action_loss: 0.7093 - val_action_loss: 0.6932 - val_decoder_MAE: 0.1746 - val_ae_action_AUC: 0.5275 - val_action_AUC: 0.5463\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.3192 - decoder_loss: 0.0280 - ae_action_loss: 0.6004 - action_loss: 0.6908 - decoder_MAE: 0.1238 - ae_action_AUC: 0.7705 - action_AUC: 0.5419 - val_loss: 1.4608 - val_decoder_loss: 0.0483 - val_ae_action_loss: 0.7191 - val_action_loss: 0.6934 - val_decoder_MAE: 0.1754 - val_ae_action_AUC: 0.5246 - val_action_AUC: 0.4919\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 1.2708 - decoder_loss: 0.0285 - ae_action_loss: 0.5618 - action_loss: 0.6805 - decoder_MAE: 0.1253 - ae_action_AUC: 0.8206 - action_AUC: 0.6161 - val_loss: 1.5104 - val_decoder_loss: 0.0495 - val_ae_action_loss: 0.7679 - val_action_loss: 0.6931 - val_decoder_MAE: 0.1781 - val_ae_action_AUC: 0.5133 - val_action_AUC: 0.5210\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.2711 - decoder_loss: 0.0280 - ae_action_loss: 0.5636 - action_loss: 0.6795 - decoder_MAE: 0.1242 - ae_action_AUC: 0.8201 - action_AUC: 0.6180 - val_loss: 1.5501 - val_decoder_loss: 0.0510 - val_ae_action_loss: 0.8063 - val_action_loss: 0.6929 - val_decoder_MAE: 0.1802 - val_ae_action_AUC: 0.4954 - val_action_AUC: 0.5190\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.2583 - decoder_loss: 0.0287 - ae_action_loss: 0.5563 - action_loss: 0.6733 - decoder_MAE: 0.1250 - ae_action_AUC: 0.8270 - action_AUC: 0.6317 - val_loss: 1.5240 - val_decoder_loss: 0.0502 - val_ae_action_loss: 0.7801 - val_action_loss: 0.6938 - val_decoder_MAE: 0.1793 - val_ae_action_AUC: 0.5265 - val_action_AUC: 0.5256\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.2085 - decoder_loss: 0.0299 - ae_action_loss: 0.5227 - action_loss: 0.6559 - decoder_MAE: 0.1282 - ae_action_AUC: 0.8552 - action_AUC: 0.6664 - val_loss: 1.5073 - val_decoder_loss: 0.0505 - val_ae_action_loss: 0.7616 - val_action_loss: 0.6951 - val_decoder_MAE: 0.1792 - val_ae_action_AUC: 0.4975 - val_action_AUC: 0.5179\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.1919 - decoder_loss: 0.0284 - ae_action_loss: 0.5298 - action_loss: 0.6337 - decoder_MAE: 0.1256 - ae_action_AUC: 0.8553 - action_AUC: 0.7240 - val_loss: 1.5814 - val_decoder_loss: 0.0537 - val_ae_action_loss: 0.8288 - val_action_loss: 0.6988 - val_decoder_MAE: 0.1842 - val_ae_action_AUC: 0.4681 - val_action_AUC: 0.5012\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.1546 - decoder_loss: 0.0287 - ae_action_loss: 0.5075 - action_loss: 0.6184 - decoder_MAE: 0.1245 - ae_action_AUC: 0.8714 - action_AUC: 0.7330 - val_loss: 1.5868 - val_decoder_loss: 0.0529 - val_ae_action_loss: 0.8363 - val_action_loss: 0.6976 - val_decoder_MAE: 0.1827 - val_ae_action_AUC: 0.5154 - val_action_AUC: 0.5188\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.1961 - decoder_loss: 0.0303 - ae_action_loss: 0.5247 - action_loss: 0.6410 - decoder_MAE: 0.1298 - ae_action_AUC: 0.8670 - action_AUC: 0.6979 - val_loss: 1.6548 - val_decoder_loss: 0.0562 - val_ae_action_loss: 0.9059 - val_action_loss: 0.6927 - val_decoder_MAE: 0.1890 - val_ae_action_AUC: 0.5375 - val_action_AUC: 0.5331\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.1633 - decoder_loss: 0.0299 - ae_action_loss: 0.5101 - action_loss: 0.6233 - decoder_MAE: 0.1277 - ae_action_AUC: 0.8768 - action_AUC: 0.7369 - val_loss: 1.7203 - val_decoder_loss: 0.0576 - val_ae_action_loss: 0.9675 - val_action_loss: 0.6952 - val_decoder_MAE: 0.1898 - val_ae_action_AUC: 0.5127 - val_action_AUC: 0.5169\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.1527 - decoder_loss: 0.0301 - ae_action_loss: 0.5018 - action_loss: 0.6209 - decoder_MAE: 0.1276 - ae_action_AUC: 0.8766 - action_AUC: 0.7402 - val_loss: 1.8394 - val_decoder_loss: 0.0586 - val_ae_action_loss: 1.0764 - val_action_loss: 0.7044 - val_decoder_MAE: 0.1939 - val_ae_action_AUC: 0.4956 - val_action_AUC: 0.5175\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.1325 - decoder_loss: 0.0289 - ae_action_loss: 0.5048 - action_loss: 0.5988 - decoder_MAE: 0.1256 - ae_action_AUC: 0.8823 - action_AUC: 0.7741 - val_loss: 1.8757 - val_decoder_loss: 0.0644 - val_ae_action_loss: 1.1010 - val_action_loss: 0.7102 - val_decoder_MAE: 0.2063 - val_ae_action_AUC: 0.5262 - val_action_AUC: 0.5294\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 1.0587 - decoder_loss: 0.0306 - ae_action_loss: 0.4685 - action_loss: 0.5596 - decoder_MAE: 0.1300 - ae_action_AUC: 0.9077 - action_AUC: 0.8242 - val_loss: 1.8765 - val_decoder_loss: 0.0671 - val_ae_action_loss: 1.1006 - val_action_loss: 0.7088 - val_decoder_MAE: 0.2096 - val_ae_action_AUC: 0.5079 - val_action_AUC: 0.5421\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 1.0371 - decoder_loss: 0.0316 - ae_action_loss: 0.4491 - action_loss: 0.5564 - decoder_MAE: 0.1314 - ae_action_AUC: 0.9235 - action_AUC: 0.8233 - val_loss: 2.5785 - val_decoder_loss: 0.0840 - val_ae_action_loss: 1.6630 - val_action_loss: 0.8316 - val_decoder_MAE: 0.2342 - val_ae_action_AUC: 0.5271 - val_action_AUC: 0.5352\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.0355 - decoder_loss: 0.0295 - ae_action_loss: 0.4544 - action_loss: 0.5516 - decoder_MAE: 0.1264 - ae_action_AUC: 0.9177 - action_AUC: 0.8316 - val_loss: 2.6993 - val_decoder_loss: 0.0876 - val_ae_action_loss: 1.7225 - val_action_loss: 0.8892 - val_decoder_MAE: 0.2413 - val_ae_action_AUC: 0.5277 - val_action_AUC: 0.5327\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 1.0137 - decoder_loss: 0.0300 - ae_action_loss: 0.4446 - action_loss: 0.5390 - decoder_MAE: 0.1290 - ae_action_AUC: 0.9207 - action_AUC: 0.8393 - val_loss: 2.4915 - val_decoder_loss: 0.0820 - val_ae_action_loss: 1.5435 - val_action_loss: 0.8661 - val_decoder_MAE: 0.2330 - val_ae_action_AUC: 0.5077 - val_action_AUC: 0.5156\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.9789 - decoder_loss: 0.0308 - ae_action_loss: 0.4164 - action_loss: 0.5317 - decoder_MAE: 0.1303 - ae_action_AUC: 0.9422 - action_AUC: 0.8466 - val_loss: 2.5166 - val_decoder_loss: 0.0826 - val_ae_action_loss: 1.5343 - val_action_loss: 0.8997 - val_decoder_MAE: 0.2330 - val_ae_action_AUC: 0.5162 - val_action_AUC: 0.5125\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.9643 - decoder_loss: 0.0319 - ae_action_loss: 0.4151 - action_loss: 0.5174 - decoder_MAE: 0.1315 - ae_action_AUC: 0.9436 - action_AUC: 0.8667 - val_loss: 3.1530 - val_decoder_loss: 0.0952 - val_ae_action_loss: 2.0911 - val_action_loss: 0.9667 - val_decoder_MAE: 0.2496 - val_ae_action_AUC: 0.5394 - val_action_AUC: 0.5396\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.9682 - decoder_loss: 0.0324 - ae_action_loss: 0.4260 - action_loss: 0.5098 - decoder_MAE: 0.1335 - ae_action_AUC: 0.9371 - action_AUC: 0.8712 - val_loss: 3.0492 - val_decoder_loss: 0.0969 - val_ae_action_loss: 1.9144 - val_action_loss: 1.0379 - val_decoder_MAE: 0.2512 - val_ae_action_AUC: 0.5325 - val_action_AUC: 0.5410\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.9035 - decoder_loss: 0.0313 - ae_action_loss: 0.3937 - action_loss: 0.4785 - decoder_MAE: 0.1312 - ae_action_AUC: 0.9528 - action_AUC: 0.8994 - val_loss: 2.8370 - val_decoder_loss: 0.0910 - val_ae_action_loss: 1.6910 - val_action_loss: 1.0550 - val_decoder_MAE: 0.2425 - val_ae_action_AUC: 0.5387 - val_action_AUC: 0.5440\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8408 - decoder_loss: 0.0322 - ae_action_loss: 0.3594 - action_loss: 0.4491 - decoder_MAE: 0.1333 - ae_action_AUC: 0.9750 - action_AUC: 0.9162 - val_loss: 2.8875 - val_decoder_loss: 0.0919 - val_ae_action_loss: 1.6684 - val_action_loss: 1.1272 - val_decoder_MAE: 0.2411 - val_ae_action_AUC: 0.5508 - val_action_AUC: 0.5471\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.8708 - decoder_loss: 0.0293 - ae_action_loss: 0.3717 - action_loss: 0.4698 - decoder_MAE: 0.1275 - ae_action_AUC: 0.9660 - action_AUC: 0.9029 - val_loss: 3.5553 - val_decoder_loss: 0.1095 - val_ae_action_loss: 2.2573 - val_action_loss: 1.1885 - val_decoder_MAE: 0.2655 - val_ae_action_AUC: 0.5710 - val_action_AUC: 0.5571\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8234 - decoder_loss: 0.0291 - ae_action_loss: 0.3527 - action_loss: 0.4416 - decoder_MAE: 0.1269 - ae_action_AUC: 0.9779 - action_AUC: 0.9251 - val_loss: 3.1033 - val_decoder_loss: 0.1000 - val_ae_action_loss: 1.8795 - val_action_loss: 1.1237 - val_decoder_MAE: 0.2534 - val_ae_action_AUC: 0.5625 - val_action_AUC: 0.5542\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8357 - decoder_loss: 0.0292 - ae_action_loss: 0.3593 - action_loss: 0.4472 - decoder_MAE: 0.1270 - ae_action_AUC: 0.9716 - action_AUC: 0.9168 - val_loss: 4.5677 - val_decoder_loss: 0.1524 - val_ae_action_loss: 3.0800 - val_action_loss: 1.3353 - val_decoder_MAE: 0.3087 - val_ae_action_AUC: 0.5012 - val_action_AUC: 0.5225\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8174 - decoder_loss: 0.0306 - ae_action_loss: 0.3678 - action_loss: 0.4189 - decoder_MAE: 0.1305 - ae_action_AUC: 0.9690 - action_AUC: 0.9361 - val_loss: 3.7234 - val_decoder_loss: 0.1365 - val_ae_action_loss: 2.3008 - val_action_loss: 1.2861 - val_decoder_MAE: 0.2914 - val_ae_action_AUC: 0.5217 - val_action_AUC: 0.5279\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8547 - decoder_loss: 0.0315 - ae_action_loss: 0.3729 - action_loss: 0.4502 - decoder_MAE: 0.1325 - ae_action_AUC: 0.9670 - action_AUC: 0.9163 - val_loss: 4.1319 - val_decoder_loss: 0.1407 - val_ae_action_loss: 2.6235 - val_action_loss: 1.3677 - val_decoder_MAE: 0.2963 - val_ae_action_AUC: 0.4779 - val_action_AUC: 0.5062\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.8200 - decoder_loss: 0.0308 - ae_action_loss: 0.3705 - action_loss: 0.4187 - decoder_MAE: 0.1310 - ae_action_AUC: 0.9665 - action_AUC: 0.9380 - val_loss: 3.3616 - val_decoder_loss: 0.1157 - val_ae_action_loss: 1.8222 - val_action_loss: 1.4238 - val_decoder_MAE: 0.2667 - val_ae_action_AUC: 0.4590 - val_action_AUC: 0.5050\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7877 - decoder_loss: 0.0301 - ae_action_loss: 0.3493 - action_loss: 0.4083 - decoder_MAE: 0.1300 - ae_action_AUC: 0.9756 - action_AUC: 0.9441 - val_loss: 4.2977 - val_decoder_loss: 0.1599 - val_ae_action_loss: 2.5872 - val_action_loss: 1.5506 - val_decoder_MAE: 0.3142 - val_ae_action_AUC: 0.4706 - val_action_AUC: 0.5081\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7823 - decoder_loss: 0.0318 - ae_action_loss: 0.3261 - action_loss: 0.4244 - decoder_MAE: 0.1339 - ae_action_AUC: 0.9867 - action_AUC: 0.9336 - val_loss: 4.3727 - val_decoder_loss: 0.1745 - val_ae_action_loss: 2.7234 - val_action_loss: 1.4748 - val_decoder_MAE: 0.3250 - val_ae_action_AUC: 0.5098 - val_action_AUC: 0.5183\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7386 - decoder_loss: 0.0315 - ae_action_loss: 0.3167 - action_loss: 0.3904 - decoder_MAE: 0.1324 - ae_action_AUC: 0.9895 - action_AUC: 0.9543 - val_loss: 3.2130 - val_decoder_loss: 0.1332 - val_ae_action_loss: 1.7546 - val_action_loss: 1.3251 - val_decoder_MAE: 0.2827 - val_ae_action_AUC: 0.5035 - val_action_AUC: 0.5358\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7480 - decoder_loss: 0.0300 - ae_action_loss: 0.3283 - action_loss: 0.3897 - decoder_MAE: 0.1296 - ae_action_AUC: 0.9826 - action_AUC: 0.9522 - val_loss: 3.9049 - val_decoder_loss: 0.1530 - val_ae_action_loss: 2.3481 - val_action_loss: 1.4038 - val_decoder_MAE: 0.3038 - val_ae_action_AUC: 0.5102 - val_action_AUC: 0.5456\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7526 - decoder_loss: 0.0306 - ae_action_loss: 0.3323 - action_loss: 0.3897 - decoder_MAE: 0.1311 - ae_action_AUC: 0.9830 - action_AUC: 0.9537 - val_loss: 4.0272 - val_decoder_loss: 0.1663 - val_ae_action_loss: 2.4139 - val_action_loss: 1.4470 - val_decoder_MAE: 0.3179 - val_ae_action_AUC: 0.5056 - val_action_AUC: 0.5463\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7277 - decoder_loss: 0.0318 - ae_action_loss: 0.3052 - action_loss: 0.3907 - decoder_MAE: 0.1344 - ae_action_AUC: 0.9903 - action_AUC: 0.9526 - val_loss: 3.1784 - val_decoder_loss: 0.1344 - val_ae_action_loss: 1.7324 - val_action_loss: 1.3115 - val_decoder_MAE: 0.2849 - val_ae_action_AUC: 0.4875 - val_action_AUC: 0.5185\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7356 - decoder_loss: 0.0297 - ae_action_loss: 0.3281 - action_loss: 0.3778 - decoder_MAE: 0.1294 - ae_action_AUC: 0.9824 - action_AUC: 0.9603 - val_loss: 3.1806 - val_decoder_loss: 0.1418 - val_ae_action_loss: 1.7086 - val_action_loss: 1.3302 - val_decoder_MAE: 0.2932 - val_ae_action_AUC: 0.5102 - val_action_AUC: 0.5229\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6937 - decoder_loss: 0.0309 - ae_action_loss: 0.3042 - action_loss: 0.3586 - decoder_MAE: 0.1315 - ae_action_AUC: 0.9913 - action_AUC: 0.9685 - val_loss: 3.2512 - val_decoder_loss: 0.1450 - val_ae_action_loss: 1.7694 - val_action_loss: 1.3368 - val_decoder_MAE: 0.2971 - val_ae_action_AUC: 0.5169 - val_action_AUC: 0.5263\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7006 - decoder_loss: 0.0318 - ae_action_loss: 0.3006 - action_loss: 0.3682 - decoder_MAE: 0.1339 - ae_action_AUC: 0.9925 - action_AUC: 0.9630 - val_loss: 3.3990 - val_decoder_loss: 0.1601 - val_ae_action_loss: 1.8604 - val_action_loss: 1.3784 - val_decoder_MAE: 0.3128 - val_ae_action_AUC: 0.5212 - val_action_AUC: 0.5435\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6951 - decoder_loss: 0.0308 - ae_action_loss: 0.3074 - action_loss: 0.3570 - decoder_MAE: 0.1308 - ae_action_AUC: 0.9920 - action_AUC: 0.9702 - val_loss: 3.0803 - val_decoder_loss: 0.1459 - val_ae_action_loss: 1.5551 - val_action_loss: 1.3793 - val_decoder_MAE: 0.2969 - val_ae_action_AUC: 0.5196 - val_action_AUC: 0.5438\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7203 - decoder_loss: 0.0295 - ae_action_loss: 0.3248 - action_loss: 0.3660 - decoder_MAE: 0.1284 - ae_action_AUC: 0.9851 - action_AUC: 0.9638 - val_loss: 4.4287 - val_decoder_loss: 0.2316 - val_ae_action_loss: 2.7905 - val_action_loss: 1.4066 - val_decoder_MAE: 0.3736 - val_ae_action_AUC: 0.5062 - val_action_AUC: 0.5058\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7114 - decoder_loss: 0.0280 - ae_action_loss: 0.3066 - action_loss: 0.3768 - decoder_MAE: 0.1249 - ae_action_AUC: 0.9902 - action_AUC: 0.9589 - val_loss: 2.8494 - val_decoder_loss: 0.1702 - val_ae_action_loss: 1.5064 - val_action_loss: 1.1729 - val_decoder_MAE: 0.3228 - val_ae_action_AUC: 0.5019 - val_action_AUC: 0.5385\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.7183 - decoder_loss: 0.0284 - ae_action_loss: 0.3142 - action_loss: 0.3757 - decoder_MAE: 0.1266 - ae_action_AUC: 0.9868 - action_AUC: 0.9574 - val_loss: 2.8484 - val_decoder_loss: 0.1550 - val_ae_action_loss: 1.5574 - val_action_loss: 1.1361 - val_decoder_MAE: 0.3069 - val_ae_action_AUC: 0.5156 - val_action_AUC: 0.5662\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7163 - decoder_loss: 0.0300 - ae_action_loss: 0.3111 - action_loss: 0.3753 - decoder_MAE: 0.1303 - ae_action_AUC: 0.9895 - action_AUC: 0.9580 - val_loss: 4.4217 - val_decoder_loss: 0.1910 - val_ae_action_loss: 2.9288 - val_action_loss: 1.3018 - val_decoder_MAE: 0.3372 - val_ae_action_AUC: 0.5469 - val_action_AUC: 0.5304\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7223 - decoder_loss: 0.0313 - ae_action_loss: 0.3249 - action_loss: 0.3661 - decoder_MAE: 0.1317 - ae_action_AUC: 0.9839 - action_AUC: 0.9644 - val_loss: 4.6792 - val_decoder_loss: 0.1828 - val_ae_action_loss: 3.1662 - val_action_loss: 1.3302 - val_decoder_MAE: 0.3312 - val_ae_action_AUC: 0.5544 - val_action_AUC: 0.5423\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6976 - decoder_loss: 0.0312 - ae_action_loss: 0.3088 - action_loss: 0.3576 - decoder_MAE: 0.1326 - ae_action_AUC: 0.9883 - action_AUC: 0.9693 - val_loss: 4.4470 - val_decoder_loss: 0.1849 - val_ae_action_loss: 2.8445 - val_action_loss: 1.4177 - val_decoder_MAE: 0.3304 - val_ae_action_AUC: 0.5098 - val_action_AUC: 0.5162\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6891 - decoder_loss: 0.0310 - ae_action_loss: 0.3049 - action_loss: 0.3533 - decoder_MAE: 0.1315 - ae_action_AUC: 0.9908 - action_AUC: 0.9680 - val_loss: 4.4518 - val_decoder_loss: 0.2026 - val_ae_action_loss: 2.7186 - val_action_loss: 1.5306 - val_decoder_MAE: 0.3470 - val_ae_action_AUC: 0.5146 - val_action_AUC: 0.5327\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6829 - decoder_loss: 0.0295 - ae_action_loss: 0.3002 - action_loss: 0.3533 - decoder_MAE: 0.1280 - ae_action_AUC: 0.9883 - action_AUC: 0.9688 - val_loss: 3.4783 - val_decoder_loss: 0.2208 - val_ae_action_loss: 1.6774 - val_action_loss: 1.5801 - val_decoder_MAE: 0.3638 - val_ae_action_AUC: 0.5183 - val_action_AUC: 0.5213\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6507 - decoder_loss: 0.0280 - ae_action_loss: 0.2920 - action_loss: 0.3307 - decoder_MAE: 0.1246 - ae_action_AUC: 0.9939 - action_AUC: 0.9777 - val_loss: 3.4915 - val_decoder_loss: 0.2263 - val_ae_action_loss: 1.6916 - val_action_loss: 1.5737 - val_decoder_MAE: 0.3697 - val_ae_action_AUC: 0.5369 - val_action_AUC: 0.5329\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6687 - decoder_loss: 0.0275 - ae_action_loss: 0.2943 - action_loss: 0.3469 - decoder_MAE: 0.1230 - ae_action_AUC: 0.9939 - action_AUC: 0.9709 - val_loss: 3.9866 - val_decoder_loss: 0.2471 - val_ae_action_loss: 2.2637 - val_action_loss: 1.4758 - val_decoder_MAE: 0.3862 - val_ae_action_AUC: 0.5383 - val_action_AUC: 0.5315\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6372 - decoder_loss: 0.0277 - ae_action_loss: 0.2829 - action_loss: 0.3265 - decoder_MAE: 0.1244 - ae_action_AUC: 0.9940 - action_AUC: 0.9808 - val_loss: 3.7124 - val_decoder_loss: 0.2529 - val_ae_action_loss: 1.9691 - val_action_loss: 1.4904 - val_decoder_MAE: 0.3918 - val_ae_action_AUC: 0.5417 - val_action_AUC: 0.5304\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6297 - decoder_loss: 0.0277 - ae_action_loss: 0.2816 - action_loss: 0.3205 - decoder_MAE: 0.1241 - ae_action_AUC: 0.9962 - action_AUC: 0.9812 - val_loss: 3.0448 - val_decoder_loss: 0.1840 - val_ae_action_loss: 1.3175 - val_action_loss: 1.5434 - val_decoder_MAE: 0.3322 - val_ae_action_AUC: 0.5427 - val_action_AUC: 0.5319\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6775 - decoder_loss: 0.0269 - ae_action_loss: 0.3014 - action_loss: 0.3493 - decoder_MAE: 0.1216 - ae_action_AUC: 0.9919 - action_AUC: 0.9699 - val_loss: 3.4076 - val_decoder_loss: 0.1933 - val_ae_action_loss: 1.7040 - val_action_loss: 1.5104 - val_decoder_MAE: 0.3389 - val_ae_action_AUC: 0.5133 - val_action_AUC: 0.4902\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6222 - decoder_loss: 0.0284 - ae_action_loss: 0.2708 - action_loss: 0.3231 - decoder_MAE: 0.1246 - ae_action_AUC: 0.9975 - action_AUC: 0.9791 - val_loss: 3.5878 - val_decoder_loss: 0.2152 - val_ae_action_loss: 1.9058 - val_action_loss: 1.4667 - val_decoder_MAE: 0.3595 - val_ae_action_AUC: 0.5033 - val_action_AUC: 0.4854\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6209 - decoder_loss: 0.0292 - ae_action_loss: 0.2738 - action_loss: 0.3180 - decoder_MAE: 0.1271 - ae_action_AUC: 0.9970 - action_AUC: 0.9823 - val_loss: 3.1019 - val_decoder_loss: 0.1958 - val_ae_action_loss: 1.5345 - val_action_loss: 1.3716 - val_decoder_MAE: 0.3456 - val_ae_action_AUC: 0.4965 - val_action_AUC: 0.5463\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6411 - decoder_loss: 0.0277 - ae_action_loss: 0.2770 - action_loss: 0.3364 - decoder_MAE: 0.1240 - ae_action_AUC: 0.9961 - action_AUC: 0.9756 - val_loss: 3.7946 - val_decoder_loss: 0.2176 - val_ae_action_loss: 2.2481 - val_action_loss: 1.3289 - val_decoder_MAE: 0.3626 - val_ae_action_AUC: 0.5331 - val_action_AUC: 0.5412\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6412 - decoder_loss: 0.0288 - ae_action_loss: 0.2851 - action_loss: 0.3273 - decoder_MAE: 0.1262 - ae_action_AUC: 0.9930 - action_AUC: 0.9786 - val_loss: 4.1206 - val_decoder_loss: 0.2392 - val_ae_action_loss: 2.4764 - val_action_loss: 1.4050 - val_decoder_MAE: 0.3797 - val_ae_action_AUC: 0.5462 - val_action_AUC: 0.5231\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6255 - decoder_loss: 0.0287 - ae_action_loss: 0.2751 - action_loss: 0.3216 - decoder_MAE: 0.1272 - ae_action_AUC: 0.9967 - action_AUC: 0.9813 - val_loss: 4.0179 - val_decoder_loss: 0.2116 - val_ae_action_loss: 2.3407 - val_action_loss: 1.4655 - val_decoder_MAE: 0.3565 - val_ae_action_AUC: 0.5429 - val_action_AUC: 0.5133\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.6253 - decoder_loss: 0.0277 - ae_action_loss: 0.2667 - action_loss: 0.3309 - decoder_MAE: 0.1239 - ae_action_AUC: 0.9977 - action_AUC: 0.9774 - val_loss: 3.1743 - val_decoder_loss: 0.1862 - val_ae_action_loss: 1.6093 - val_action_loss: 1.3787 - val_decoder_MAE: 0.3349 - val_ae_action_AUC: 0.5365 - val_action_AUC: 0.5788\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.6192 - decoder_loss: 0.0281 - ae_action_loss: 0.2695 - action_loss: 0.3216 - decoder_MAE: 0.1250 - ae_action_AUC: 0.9974 - action_AUC: 0.9800 - val_loss: 3.0282 - val_decoder_loss: 0.2026 - val_ae_action_loss: 1.5102 - val_action_loss: 1.3154 - val_decoder_MAE: 0.3498 - val_ae_action_AUC: 0.5583 - val_action_AUC: 0.5875\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.6360 - decoder_loss: 0.0269 - ae_action_loss: 0.2917 - action_loss: 0.3173 - decoder_MAE: 0.1223 - ae_action_AUC: 0.9916 - action_AUC: 0.9819 - val_loss: 3.0969 - val_decoder_loss: 0.2003 - val_ae_action_loss: 1.5111 - val_action_loss: 1.3856 - val_decoder_MAE: 0.3480 - val_ae_action_AUC: 0.5421 - val_action_AUC: 0.5912\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6173 - decoder_loss: 0.0264 - ae_action_loss: 0.2740 - action_loss: 0.3169 - decoder_MAE: 0.1205 - ae_action_AUC: 0.9958 - action_AUC: 0.9823 - val_loss: 3.2930 - val_decoder_loss: 0.1943 - val_ae_action_loss: 1.6059 - val_action_loss: 1.4929 - val_decoder_MAE: 0.3427 - val_ae_action_AUC: 0.5196 - val_action_AUC: 0.5638\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6023 - decoder_loss: 0.0267 - ae_action_loss: 0.2730 - action_loss: 0.3027 - decoder_MAE: 0.1213 - ae_action_AUC: 0.9978 - action_AUC: 0.9863 - val_loss: 3.0545 - val_decoder_loss: 0.1827 - val_ae_action_loss: 1.4088 - val_action_loss: 1.4630 - val_decoder_MAE: 0.3316 - val_ae_action_AUC: 0.5269 - val_action_AUC: 0.5412\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.6080 - decoder_loss: 0.0264 - ae_action_loss: 0.2724 - action_loss: 0.3092 - decoder_MAE: 0.1213 - ae_action_AUC: 0.9971 - action_AUC: 0.9855 - val_loss: 3.2652 - val_decoder_loss: 0.1650 - val_ae_action_loss: 1.7017 - val_action_loss: 1.3984 - val_decoder_MAE: 0.3146 - val_ae_action_AUC: 0.5388 - val_action_AUC: 0.5333\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5800 - decoder_loss: 0.0274 - ae_action_loss: 0.2624 - action_loss: 0.2902 - decoder_MAE: 0.1239 - ae_action_AUC: 0.9992 - action_AUC: 0.9925 - val_loss: 2.8482 - val_decoder_loss: 0.1681 - val_ae_action_loss: 1.2767 - val_action_loss: 1.4034 - val_decoder_MAE: 0.3194 - val_ae_action_AUC: 0.5546 - val_action_AUC: 0.5483\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6134 - decoder_loss: 0.0267 - ae_action_loss: 0.2679 - action_loss: 0.3189 - decoder_MAE: 0.1216 - ae_action_AUC: 0.9980 - action_AUC: 0.9820 - val_loss: 2.8789 - val_decoder_loss: 0.1761 - val_ae_action_loss: 1.3220 - val_action_loss: 1.3808 - val_decoder_MAE: 0.3274 - val_ae_action_AUC: 0.5444 - val_action_AUC: 0.5537\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6214 - decoder_loss: 0.0260 - ae_action_loss: 0.2799 - action_loss: 0.3155 - decoder_MAE: 0.1202 - ae_action_AUC: 0.9941 - action_AUC: 0.9806 - val_loss: 2.9084 - val_decoder_loss: 0.1954 - val_ae_action_loss: 1.3697 - val_action_loss: 1.3433 - val_decoder_MAE: 0.3470 - val_ae_action_AUC: 0.5458 - val_action_AUC: 0.5448\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6470 - decoder_loss: 0.0264 - ae_action_loss: 0.2969 - action_loss: 0.3238 - decoder_MAE: 0.1206 - ae_action_AUC: 0.9906 - action_AUC: 0.9807 - val_loss: 3.4955 - val_decoder_loss: 0.2049 - val_ae_action_loss: 1.9006 - val_action_loss: 1.3900 - val_decoder_MAE: 0.3524 - val_ae_action_AUC: 0.5462 - val_action_AUC: 0.5313\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.5988 - decoder_loss: 0.0280 - ae_action_loss: 0.2697 - action_loss: 0.3011 - decoder_MAE: 0.1246 - ae_action_AUC: 0.9969 - action_AUC: 0.9883 - val_loss: 3.4702 - val_decoder_loss: 0.2157 - val_ae_action_loss: 1.7799 - val_action_loss: 1.4746 - val_decoder_MAE: 0.3617 - val_ae_action_AUC: 0.5258 - val_action_AUC: 0.5294\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6055 - decoder_loss: 0.0267 - ae_action_loss: 0.2757 - action_loss: 0.3031 - decoder_MAE: 0.1216 - ae_action_AUC: 0.9960 - action_AUC: 0.9877 - val_loss: 3.2830 - val_decoder_loss: 0.2074 - val_ae_action_loss: 1.5041 - val_action_loss: 1.5715 - val_decoder_MAE: 0.3557 - val_ae_action_AUC: 0.5527 - val_action_AUC: 0.5390\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6020 - decoder_loss: 0.0269 - ae_action_loss: 0.2653 - action_loss: 0.3098 - decoder_MAE: 0.1225 - ae_action_AUC: 0.9968 - action_AUC: 0.9832 - val_loss: 3.4125 - val_decoder_loss: 0.2373 - val_ae_action_loss: 1.7361 - val_action_loss: 1.4391 - val_decoder_MAE: 0.3793 - val_ae_action_AUC: 0.5377 - val_action_AUC: 0.5160\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6157 - decoder_loss: 0.0279 - ae_action_loss: 0.2769 - action_loss: 0.3109 - decoder_MAE: 0.1246 - ae_action_AUC: 0.9953 - action_AUC: 0.9834 - val_loss: 2.9876 - val_decoder_loss: 0.2411 - val_ae_action_loss: 1.3597 - val_action_loss: 1.3869 - val_decoder_MAE: 0.3842 - val_ae_action_AUC: 0.5671 - val_action_AUC: 0.4856\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6192 - decoder_loss: 0.0265 - ae_action_loss: 0.2738 - action_loss: 0.3189 - decoder_MAE: 0.1208 - ae_action_AUC: 0.9969 - action_AUC: 0.9817 - val_loss: 3.2365 - val_decoder_loss: 0.2599 - val_ae_action_loss: 1.5163 - val_action_loss: 1.4603 - val_decoder_MAE: 0.3969 - val_ae_action_AUC: 0.5627 - val_action_AUC: 0.5252\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.6052 - decoder_loss: 0.0262 - ae_action_loss: 0.2670 - action_loss: 0.3120 - decoder_MAE: 0.1199 - ae_action_AUC: 0.9974 - action_AUC: 0.9824 - val_loss: 3.2822 - val_decoder_loss: 0.2714 - val_ae_action_loss: 1.5997 - val_action_loss: 1.4111 - val_decoder_MAE: 0.4044 - val_ae_action_AUC: 0.5294 - val_action_AUC: 0.5110\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.5766 - decoder_loss: 0.0266 - ae_action_loss: 0.2568 - action_loss: 0.2931 - decoder_MAE: 0.1207 - ae_action_AUC: 0.9993 - action_AUC: 0.9894 - val_loss: 2.9653 - val_decoder_loss: 0.2528 - val_ae_action_loss: 1.3041 - val_action_loss: 1.4083 - val_decoder_MAE: 0.3886 - val_ae_action_AUC: 0.5477 - val_action_AUC: 0.5148\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6270 - decoder_loss: 0.0260 - ae_action_loss: 0.2748 - action_loss: 0.3262 - decoder_MAE: 0.1186 - ae_action_AUC: 0.9952 - action_AUC: 0.9783 - val_loss: 3.0983 - val_decoder_loss: 0.2226 - val_ae_action_loss: 1.5150 - val_action_loss: 1.3607 - val_decoder_MAE: 0.3626 - val_ae_action_AUC: 0.5246 - val_action_AUC: 0.4662\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.5796 - decoder_loss: 0.0242 - ae_action_loss: 0.2601 - action_loss: 0.2953 - decoder_MAE: 0.1148 - ae_action_AUC: 0.9989 - action_AUC: 0.9885 - val_loss: 3.2873 - val_decoder_loss: 0.2184 - val_ae_action_loss: 1.7436 - val_action_loss: 1.3253 - val_decoder_MAE: 0.3587 - val_ae_action_AUC: 0.5219 - val_action_AUC: 0.4962\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6136 - decoder_loss: 0.0250 - ae_action_loss: 0.2747 - action_loss: 0.3140 - decoder_MAE: 0.1174 - ae_action_AUC: 0.9952 - action_AUC: 0.9816 - val_loss: 3.3965 - val_decoder_loss: 0.2298 - val_ae_action_loss: 1.7849 - val_action_loss: 1.3818 - val_decoder_MAE: 0.3677 - val_ae_action_AUC: 0.5204 - val_action_AUC: 0.4906\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6028 - decoder_loss: 0.0245 - ae_action_loss: 0.2709 - action_loss: 0.3073 - decoder_MAE: 0.1163 - ae_action_AUC: 0.9959 - action_AUC: 0.9843 - val_loss: 4.1745 - val_decoder_loss: 0.2710 - val_ae_action_loss: 2.4280 - val_action_loss: 1.4755 - val_decoder_MAE: 0.3966 - val_ae_action_AUC: 0.5100 - val_action_AUC: 0.4990\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5906 - decoder_loss: 0.0258 - ae_action_loss: 0.2713 - action_loss: 0.2935 - decoder_MAE: 0.1191 - ae_action_AUC: 0.9968 - action_AUC: 0.9906 - val_loss: 3.4959 - val_decoder_loss: 0.2668 - val_ae_action_loss: 1.7280 - val_action_loss: 1.5011 - val_decoder_MAE: 0.3979 - val_ae_action_AUC: 0.5206 - val_action_AUC: 0.5096\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.5841 - decoder_loss: 0.0268 - ae_action_loss: 0.2712 - action_loss: 0.2861 - decoder_MAE: 0.1218 - ae_action_AUC: 0.9964 - action_AUC: 0.9929 - val_loss: 4.2708 - val_decoder_loss: 0.3008 - val_ae_action_loss: 2.4644 - val_action_loss: 1.5056 - val_decoder_MAE: 0.4174 - val_ae_action_AUC: 0.5254 - val_action_AUC: 0.5129\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.5990 - decoder_loss: 0.0265 - ae_action_loss: 0.2731 - action_loss: 0.2994 - decoder_MAE: 0.1208 - ae_action_AUC: 0.9960 - action_AUC: 0.9874 - val_loss: 3.6023 - val_decoder_loss: 0.3136 - val_ae_action_loss: 1.8812 - val_action_loss: 1.4075 - val_decoder_MAE: 0.4294 - val_ae_action_AUC: 0.5210 - val_action_AUC: 0.4873\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5909 - decoder_loss: 0.0250 - ae_action_loss: 0.2644 - action_loss: 0.3016 - decoder_MAE: 0.1168 - ae_action_AUC: 0.9969 - action_AUC: 0.9871 - val_loss: 3.8570 - val_decoder_loss: 0.3123 - val_ae_action_loss: 2.2097 - val_action_loss: 1.3350 - val_decoder_MAE: 0.4274 - val_ae_action_AUC: 0.5190 - val_action_AUC: 0.4912\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.6169 - decoder_loss: 0.0260 - ae_action_loss: 0.2730 - action_loss: 0.3179 - decoder_MAE: 0.1179 - ae_action_AUC: 0.9947 - action_AUC: 0.9797 - val_loss: 3.1327 - val_decoder_loss: 0.2785 - val_ae_action_loss: 1.5726 - val_action_loss: 1.2816 - val_decoder_MAE: 0.4064 - val_ae_action_AUC: 0.5448 - val_action_AUC: 0.4835\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.5920 - decoder_loss: 0.0259 - ae_action_loss: 0.2653 - action_loss: 0.3008 - decoder_MAE: 0.1194 - ae_action_AUC: 0.9983 - action_AUC: 0.9835 - val_loss: 3.3714 - val_decoder_loss: 0.2825 - val_ae_action_loss: 1.6922 - val_action_loss: 1.3967 - val_decoder_MAE: 0.4093 - val_ae_action_AUC: 0.5279 - val_action_AUC: 0.4687\n",
      "AUC: 0.5911538600921631\n"
     ]
    }
   ],
   "source": [
    "def create_model(X_shape, hidden_units, dropout_rates, lr, ls):\n",
    "\n",
    "    # tf.random.set_seed(87)\n",
    "\n",
    "    inp = Input(shape = (X_shape[1], ))\n",
    "    x0 = BatchNormalization()(inp)\n",
    "\n",
    "    encoder = GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = Dense(hidden_units[0])(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = Dense(X_shape[1], name = 'decoder')(decoder)  \n",
    "\n",
    "    x_ae = Dense(hidden_units[1])(decoder)\n",
    "    x_ae = BatchNormalization()(x_ae)\n",
    "    x_ae = Activation('swish')(x_ae)\n",
    "    x_ae = Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "    out_ae = Dense(1, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "    \n",
    "    x = Concatenate()([x0, encoder])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[3])(x)\n",
    "\n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = Dense(hidden_units[i])(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    out = Dense(1, activation = 'sigmoid', name = 'action')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=[decoder, out_ae, out])\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n",
    "                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls),\n",
    "                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                         },\n",
    "                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name='MAE'), \n",
    "                             'ae_action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                             'action': tf.keras.metrics.AUC(name='AUC'), \n",
    "                            }, \n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "if TUNNING == False:\n",
    "\n",
    "    path = f'model.hdf5'\n",
    "    model = create_model(**params)\n",
    "    ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=50, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    history = model.fit(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],  # full_X_resampled, [full_X_resampled, full_y_resampled, full_y_resampled]\n",
    "                        validation_data=(valid_X, [valid_X, valid_y, valid_y]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "                        # sample_weight = sw[tr], \n",
    "                        epochs=100,                                                       # 100 or coose epochs=n by experience\n",
    "                        # batch_size=16,\n",
    "                        batch_size=32,                          \n",
    "                        callbacks=[ckp, es],                                              # If you want to use, uncomment\n",
    "                        verbose=1)\n",
    "    \n",
    "    tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    score = hist['val_action_AUC'].max()\n",
    "    print(f'AUC:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.Test model on one stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 63ms/step\n",
      "ACC: 0.5714285714285714\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pred  True\n",
       "0      0   1.0\n",
       "1      0   1.0\n",
       "2      1   0.0\n",
       "3      1   0.0\n",
       "4      0   1.0\n",
       "5      1   1.0\n",
       "6      1   1.0\n",
       "7      1   1.0\n",
       "8      0   1.0\n",
       "9      0   1.0\n",
       "10     1   1.0\n",
       "11     0   0.0\n",
       "12     0   1.0\n",
       "13     0   0.0\n",
       "14     0   0.0\n",
       "15     1   0.0\n",
       "16     0   1.0\n",
       "17     1   0.0\n",
       "18     1   1.0\n",
       "19     0   0.0\n",
       "20     0   0.0\n",
       "21     0   1.0\n",
       "22     0   0.0\n",
       "23     1   0.0\n",
       "24     1   1.0\n",
       "25     1   0.0\n",
       "26     1   1.0\n",
       "27     1   0.0\n",
       "28     1   0.0\n",
       "29     0   0.0\n",
       "30     1   1.0\n",
       "31     0   1.0\n",
       "32     1   1.0\n",
       "33     0   0.0\n",
       "34     1   0.0\n",
       "35     0   1.0\n",
       "36     1   1.0\n",
       "37     1   1.0\n",
       "38     0   0.0\n",
       "39     0   1.0\n",
       "40     1   1.0\n",
       "41     0   1.0\n",
       "42     0   0.0\n",
       "43     0   0.0\n",
       "44     0   1.0\n",
       "45     0   0.0\n",
       "46     0   0.0\n",
       "47     0   0.0\n",
       "48     0   0.0\n",
       "49     1   0.0\n",
       "50     1   1.0\n",
       "51     0   0.0\n",
       "52     0   1.0\n",
       "53     1   1.0\n",
       "54     1   0.0\n",
       "55     0   0.0\n",
       "56     1   0.0\n",
       "57     0   0.0\n",
       "58     1   0.0\n",
       "59     1   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = model.predict(test_X) \n",
    "pred_dir = pred_dir[2]\n",
    "pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "result_df['True'] = test_y\n",
    "\n",
    "match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "correct = match_count / len(result_df)\n",
    "\n",
    "print(f'ACC: {correct}\\n')\n",
    "result_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.Sanity Check: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 10ms/step\n",
      "ACC: 0.9978401727861771\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pred  True\n",
       "0      1   1.0\n",
       "1      1   1.0\n",
       "2      0   0.0\n",
       "3      0   0.0\n",
       "4      1   1.0\n",
       "5      1   1.0\n",
       "6      0   0.0\n",
       "7      0   0.0\n",
       "8      0   0.0\n",
       "9      0   0.0\n",
       "10     0   0.0\n",
       "11     1   1.0\n",
       "12     0   0.0\n",
       "13     1   1.0\n",
       "14     1   1.0\n",
       "15     0   0.0\n",
       "16     1   1.0\n",
       "17     0   0.0\n",
       "18     0   0.0\n",
       "19     1   1.0\n",
       "20     1   1.0\n",
       "21     1   1.0\n",
       "22     0   0.0\n",
       "23     1   1.0\n",
       "24     0   0.0\n",
       "25     0   0.0\n",
       "26     0   0.0\n",
       "27     1   1.0\n",
       "28     1   1.0\n",
       "29     0   0.0\n",
       "30     0   0.0\n",
       "31     1   1.0\n",
       "32     1   1.0\n",
       "33     0   0.0\n",
       "34     0   0.0\n",
       "35     1   1.0\n",
       "36     1   1.0\n",
       "37     1   1.0\n",
       "38     1   1.0\n",
       "39     0   0.0\n",
       "40     0   0.0\n",
       "41     1   1.0\n",
       "42     1   1.0\n",
       "43     1   1.0\n",
       "44     0   0.0\n",
       "45     0   0.0\n",
       "46     0   0.0\n",
       "47     0   0.0\n",
       "48     0   0.0\n",
       "49     1   1.0\n",
       "50     1   1.0\n",
       "51     1   1.0\n",
       "52     0   0.0\n",
       "53     0   0.0\n",
       "54     0   0.0\n",
       "55     1   1.0\n",
       "56     0   0.0\n",
       "57     1   1.0\n",
       "58     1   1.0\n",
       "59     0   0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = model.predict(train_X) \n",
    "pred_dir = pred_dir[2]\n",
    "pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "result_df['True'] = train_y\n",
    "\n",
    "match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "correct = match_count / len(result_df)\n",
    "\n",
    "print(f'ACC: {correct}\\n')\n",
    "result_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.Sanity Check: valid (Why 0.549 but not 0.625?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 31ms/step\n",
      "ACC: 0.4803921568627451\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pred  True\n",
       "0      1   0.0\n",
       "1      0   1.0\n",
       "2      0   1.0\n",
       "3      1   1.0\n",
       "4      1   0.0\n",
       "5      0   0.0\n",
       "6      1   1.0\n",
       "7      1   1.0\n",
       "8      0   1.0\n",
       "9      1   0.0\n",
       "10     0   1.0\n",
       "11     1   0.0\n",
       "12     1   1.0\n",
       "13     1   0.0\n",
       "14     1   0.0\n",
       "15     1   0.0\n",
       "16     1   0.0\n",
       "17     1   1.0\n",
       "18     1   0.0\n",
       "19     1   1.0\n",
       "20     1   0.0\n",
       "21     1   0.0\n",
       "22     1   1.0\n",
       "23     1   1.0\n",
       "24     1   0.0\n",
       "25     1   1.0\n",
       "26     1   0.0\n",
       "27     1   1.0\n",
       "28     1   0.0\n",
       "29     1   1.0\n",
       "30     1   0.0\n",
       "31     1   1.0\n",
       "32     1   0.0\n",
       "33     1   1.0\n",
       "34     1   1.0\n",
       "35     1   1.0\n",
       "36     1   1.0\n",
       "37     1   0.0\n",
       "38     1   0.0\n",
       "39     1   1.0\n",
       "40     1   1.0\n",
       "41     1   0.0\n",
       "42     1   0.0\n",
       "43     1   1.0\n",
       "44     1   0.0\n",
       "45     1   0.0\n",
       "46     1   0.0\n",
       "47     1   0.0\n",
       "48     1   1.0\n",
       "49     1   1.0\n",
       "50     1   0.0\n",
       "51     1   0.0\n",
       "52     1   1.0\n",
       "53     1   0.0\n",
       "54     1   0.0\n",
       "55     1   0.0\n",
       "56     1   0.0\n",
       "57     1   1.0\n",
       "58     1   1.0\n",
       "59     1   0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = model.predict(valid_X) \n",
    "pred_dir = pred_dir[2]\n",
    "pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "result_df['True'] = valid_y\n",
    "\n",
    "match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "correct = match_count / len(result_df)\n",
    "\n",
    "print(f'ACC: {correct}\\n')\n",
    "result_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############\n",
    "# TP = 1\n",
    "# TUNNING = True\n",
    "# params = {'X_shape': train_X.shape,\n",
    "#           'hidden_units': [32, 64, 16, 32, 16], \n",
    "#           'dropout_rates': [0.2, 0.8, 0.5, 0.8, 0.2, 0.0, 0.2],\n",
    "#           'ls': 0.001, 'lr': 0.01}\n",
    "\n",
    "# # constituent = [2330, 2454, 2317, 2308, 2382, 2303, 2891, 3711, 2881, 2412,\n",
    "# #                2886, 2882, 2884, 1216, 2885, 3231, 3034, 2357, 2002, 2892,\n",
    "# #                1303, 2379, 5880, 2301, 3037, 2345, 1301, 3008, 3661, 2890,\n",
    "# #                5871, 2880, 2327, 2883, 2887, 2207, 4938, 1101, 6669, 1326,\n",
    "# #                2395, 3045, 5876, 2603, 1590, 2912, 4904, 2801, 6505, 2408]\n",
    "# constituent = [2330, 2454, 2317, 2308, 2382, 2303, 2891, 3711, 2881, 2412] # just test\n",
    "# #############\n",
    "\n",
    "# experiment_0050_result = pd.DataFrame()\n",
    "\n",
    "# for TICKER in constituent:\n",
    "\n",
    "#     print(f'\\n now: processing {TICKER} \\n')\n",
    "\n",
    "#     try:\n",
    "#         ##### import data #####\n",
    "#         data = pd.read_csv('/Users/yitsung/Desktop/MasterThesis/data/TaiwanStockData_Top100_EMA')\n",
    "#         ticker_data = data[data['ticker']==TICKER].reset_index(drop=True)\n",
    "#         ticker_data = ticker_data.drop(columns=['ticker'])\n",
    "\n",
    "#         ticker_data[f'y_{TP}'] = ticker_data['close'].rolling(window=TP).mean()\n",
    "#         ticker_data[f'y_{TP}'] = ticker_data[f'y_{TP}'].shift(-TP)\n",
    "#         ticker_data = ticker_data.dropna().reindex()\n",
    "#         ticker_data[f'y_{TP}'] = ((ticker_data[f'y_{TP}'] - ticker_data['close']) >= 0).astype(int)\n",
    "\n",
    "#         ##### Splite data into train(Library) and test(Prediction) #####\n",
    "#         Library = ticker_data[ticker_data['Date'] <= '2023-06-30']\n",
    "#         Prediction = ticker_data[(ticker_data['Date'] >= '2023-06-01')&(ticker_data['Date'] <= '2023-10-31')]\n",
    "\n",
    "#         ##### Data Normalize #####\n",
    "#         train_Library = Library[: int((len(Library) * 0.8))]\n",
    "#         valid_Library = Library[int((len(Library) * 0.8)): ]\n",
    "\n",
    "#         train_Library, valid_Library = make_data_minmax(Library=train_Library, Prediction=valid_Library)\n",
    "#         Library, Prediction = make_data_minmax(Library=Library, Prediction=Prediction)\n",
    "\n",
    "#         ##### Make window data: X, y #####\n",
    "#         train_X, train_y = data_preprocess(data=train_Library, window_size=20)\n",
    "#         valid_X, valid_y = data_preprocess(data=valid_Library, window_size=20)\n",
    "#         test_X, test_y = data_preprocess(data=Prediction, window_size=20)\n",
    "\n",
    "#         ##### Flatten(MLP only) #####\n",
    "#         train_X = make_X_flatten(train_X)\n",
    "#         valid_X = make_X_flatten(valid_X)\n",
    "#         test_X = make_X_flatten(test_X)\n",
    "\n",
    "#         ##### Over-smapling #####\n",
    "#         ros = RandomOverSampler(random_state=87)\n",
    "#         train_X_resampled, train_y_resampled = ros.fit_resample(train_X, train_y)\n",
    "#         train_y_resampled = train_y_resampled.reshape(-1,1) # just test\n",
    "\n",
    "#         ###### Create model and find hyperparameter #####\n",
    "#         if TUNNING:\n",
    "#             model_fn = lambda hp: tunning_model(hp, X_shape=train_X.shape)\n",
    "#             tuner = kt.BayesianOptimization(model_fn,\n",
    "#                                             objective=kt.Objective(\"val_action_AUC\", direction=\"max\"),\n",
    "#                                             max_trials=10,\n",
    "#                                             executions_per_trial=1,\n",
    "#                                             directory=\"model_kt\",\n",
    "#                                             overwrite=True,\n",
    "#                                             seed=87)\n",
    "#             path = f'model.hdf5'\n",
    "#             ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "#                                 save_best_only=True, save_weights_only=True, mode='max')\n",
    "#             es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=30, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "#                             baseline=None, restore_best_weights=True, verbose=1)\n",
    "            \n",
    "#             tuner.search(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],\n",
    "#                         validation_data=(valid_X, [valid_X, valid_y, valid_y]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "#                         epochs=100,                                                               # 100 or coose epochs=n by experience \n",
    "#                         batch_size=16, \n",
    "#                         callbacks=[ckp, es],                                                      # If you want to use, uncomment \n",
    "#                         verbose=1)\n",
    "            \n",
    "#             model = tuner.get_best_models()[0]\n",
    "\n",
    "#             tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "#             best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "#             print(\"Best Hyperparameters:\")\n",
    "#             print(best_hyperparameters.values)\n",
    "\n",
    "#         ##### Train model(with parameter) #####\n",
    "#         else:\n",
    "#             path = f'model.hdf5'\n",
    "#             model = create_model(**params)\n",
    "#             ckp = ModelCheckpoint(path, monitor='val_action_AUC', verbose = 0,                    # If you want to use, uncomment\n",
    "#                                 save_best_only=True, save_weights_only=True, mode='max')\n",
    "#             es = EarlyStopping(monitor='val_action_AUC', min_delta=1e-4, patience=30, mode='max', # If you want to use, uncomment # or choose patience=n by experience\n",
    "#                             baseline=None, restore_best_weights=True, verbose=1)\n",
    "            \n",
    "#             history = model.fit(train_X_resampled, [train_X_resampled, train_y_resampled, train_y_resampled],  # full_X_resampled, [full_X_resampled, full_y_resampled, full_y_resampled]\n",
    "#                                 validation_data=(valid_X, [valid_X, valid_y, valid_y]), # validation_data=(valid_X, [valid_X, valid_y, valid_y]) # validation_split=0.2, shuffle=True\n",
    "#                                 # sample_weight = sw[tr], \n",
    "#                                 epochs=100,                                                       # 100 or coose epochs=n by experience\n",
    "#                                 batch_size=16, \n",
    "#                                 callbacks=[ckp, es],                                              # If you want to use, uncomment\n",
    "#                                 verbose=1)\n",
    "            \n",
    "#             hist = pd.DataFrame(history.history)\n",
    "#             score = hist['val_action_AUC'].max()\n",
    "#             print(f'AUC:', score)\n",
    "\n",
    "#         ##### Test model on one stock #####\n",
    "#         pred_dir = model.predict(test_X) \n",
    "#         pred_dir = pred_dir[2]\n",
    "#         pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "#         result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "#         result_df['True'] = test_y\n",
    "\n",
    "#         match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "#         correct = match_count / len(result_df)\n",
    "#         print(f'\\n ACC: {correct} \\n')\n",
    "\n",
    "#         tf.keras.backend.clear_session() # clear memory\n",
    "\n",
    "#         ##### Add to result dataframe #####\n",
    "#         experiment_0050_result = pd.concat([experiment_0050_result, result_df], axis=0, ignore_index=True)\n",
    "\n",
    "#     except:\n",
    "#         print(f'{TICKER} process failed.')\n",
    "#         continue\n",
    "\n",
    "# #### Final ACC ####\n",
    "# whole_match_count = (experiment_0050_result['Pred'] == experiment_0050_result['True']).sum()\n",
    "# whole_correct = whole_match_count / len(experiment_0050_result)\n",
    "# print(f'\\n Whole 0050 ACC: {whole_correct} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_0050_result.to_csv(f'AE-MLP_Tp={TP}_result.csv', index=False)\n",
    "# print(f'\\n Whole 0050 ACC: {whole_correct} \\n')\n",
    "\n",
    "# experiment_0050_result.tail(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
