{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(87)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer, Dot, RepeatVector, Activation, Add, Lambda, Concatenate\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# KERAS_ATTENTION_DEBUG: If set to 1. Will switch to debug mode.\n",
    "# In debug mode, the class Attention is no longer a Keras layer.\n",
    "# What it means in practice is that we can have access to the internal values\n",
    "# of each tensor. If we don't use debug, Keras treats the object\n",
    "# as a layer, and we can only get the final output.\n",
    "debug_flag = int(os.environ.get('KERAS_ATTENTION_DEBUG', 0))\n",
    "\n",
    "\n",
    "# References:\n",
    "# - https://arxiv.org/pdf/1508.04025.pdf (Luong).\n",
    "# - https://arxiv.org/pdf/1409.0473.pdf (Bahdanau).\n",
    "# - https://machinelearningmastery.com/the-bahdanau-attention-mechanism/ (Some more explanation).\n",
    "\n",
    "class Attention(object if debug_flag else Layer):\n",
    "    SCORE_LUONG = 'luong'\n",
    "    SCORE_BAHDANAU = 'bahdanau'\n",
    "\n",
    "    def __init__(self, units: int = 128, score: str = 'luong', **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        if score not in {self.SCORE_LUONG, self.SCORE_BAHDANAU}:\n",
    "            raise ValueError(f'Possible values for score are: [{self.SCORE_LUONG}] and [{self.SCORE_BAHDANAU}].')\n",
    "        self.units = units\n",
    "        self.score = score\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        with K.name_scope(self.name if not debug_flag else 'attention'):\n",
    "            # W in W*h_S.\n",
    "            if self.score == self.SCORE_LUONG:\n",
    "                self.luong_w = Dense(input_dim, use_bias=False, name='luong_w')\n",
    "                # dot : last hidden state H_t and every hidden state H_s.\n",
    "                self.luong_dot = Dot(axes=[1, 2], name='attention_score')\n",
    "            else:\n",
    "                # Dense implements the operation: output = activation(dot(input, kernel) + bias)\n",
    "                self.bahdanau_v = Dense(1, use_bias=False, name='bahdanau_v')\n",
    "                self.bahdanau_w1 = Dense(input_dim, use_bias=False, name='bahdanau_w1')\n",
    "                self.bahdanau_w2 = Dense(input_dim, use_bias=False, name='bahdanau_w2')\n",
    "                self.bahdanau_repeat = RepeatVector(input_shape[1])\n",
    "                self.bahdanau_tanh = Activation('tanh', name='bahdanau_tanh')\n",
    "                self.bahdanau_add = Add()\n",
    "\n",
    "            self.h_t = Lambda(lambda x: x[:, -1, :], output_shape=(input_dim,), name='last_hidden_state')\n",
    "\n",
    "            # exp / sum(exp) -> softmax.\n",
    "            self.softmax_normalizer = Activation('softmax', name='attention_weight')\n",
    "\n",
    "            # dot : score * every hidden state H_s.\n",
    "            # dot product. SUM(v1*v2). H_s = every source hidden state.\n",
    "            self.dot_context = Dot(axes=[1, 1], name='context_vector')\n",
    "\n",
    "            # [Ct; ht]\n",
    "            self.concat_c_h = Concatenate(name='attention_output')\n",
    "\n",
    "            # x -> tanh(w_c(x))\n",
    "            self.w_c = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')\n",
    "        if not debug_flag:\n",
    "            # debug: the call to build() is done in call().\n",
    "            super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.units\n",
    "\n",
    "    def __call__(self, inputs, training=None, **kwargs):\n",
    "        if debug_flag:\n",
    "            return self.call(inputs, training, **kwargs)\n",
    "        else:\n",
    "            return super(Attention, self).__call__(inputs, training, **kwargs)\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Many-to-one attention mechanism for Keras. Supports:\n",
    "            - Luong's multiplicative style.\n",
    "            - Bahdanau's additive style.\n",
    "        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "        @param training: not used in this layer.\n",
    "        @return: 2D tensor with shape (batch_size, units)\n",
    "        @author: philipperemy, felixhao28.\n",
    "        \"\"\"\n",
    "        h_s = inputs\n",
    "        if debug_flag:\n",
    "            self.build(h_s.shape)\n",
    "        h_t = self.h_t(h_s)\n",
    "        if self.score == self.SCORE_LUONG:\n",
    "            # Luong's multiplicative style.\n",
    "            score = self.luong_dot([h_t, self.luong_w(h_s)])\n",
    "        else:\n",
    "            # Bahdanau's additive style.\n",
    "            self.bahdanau_w1(h_s)\n",
    "            a1 = self.bahdanau_w1(h_t)\n",
    "            a2 = self.bahdanau_w2(h_s)\n",
    "            a1 = self.bahdanau_repeat(a1)\n",
    "            score = self.bahdanau_tanh(self.bahdanau_add([a1, a2]))\n",
    "            score = self.bahdanau_v(score)\n",
    "            score = K.squeeze(score, axis=-1)\n",
    "\n",
    "        alpha_s = self.softmax_normalizer(score)\n",
    "        context_vector = self.dot_context([h_s, alpha_s])\n",
    "        a_t = self.w_c(self.concat_c_h([context_vector, h_t]))\n",
    "        return a_t\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Attention, self).get_config()\n",
    "        config.update({'units': self.units, 'score': self.score})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>financing</th>\n",
       "      <th>fi</th>\n",
       "      <th>ii</th>\n",
       "      <th>di</th>\n",
       "      <th>rp</th>\n",
       "      <th>capital</th>\n",
       "      <th>EMA9</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>y_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>530.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>39490.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>12463.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>2342.0</td>\n",
       "      <td>6.0443</td>\n",
       "      <td>521.295251</td>\n",
       "      <td>518.980386</td>\n",
       "      <td>513.251221</td>\n",
       "      <td>5.729165</td>\n",
       "      <td>3.933239</td>\n",
       "      <td>84.477581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>536.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>34839.0</td>\n",
       "      <td>-355.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>-451.0</td>\n",
       "      <td>-1374.0</td>\n",
       "      <td>5.3592</td>\n",
       "      <td>525.437881</td>\n",
       "      <td>522.532126</td>\n",
       "      <td>515.535238</td>\n",
       "      <td>6.996887</td>\n",
       "      <td>4.619674</td>\n",
       "      <td>88.417310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>555.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>55614.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-4163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>530.151835</td>\n",
       "      <td>526.614084</td>\n",
       "      <td>518.179719</td>\n",
       "      <td>8.434365</td>\n",
       "      <td>5.454306</td>\n",
       "      <td>91.005801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>554.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>53393.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>-402.0</td>\n",
       "      <td>8.7664</td>\n",
       "      <td>537.123278</td>\n",
       "      <td>532.531850</td>\n",
       "      <td>521.861371</td>\n",
       "      <td>10.670478</td>\n",
       "      <td>6.574521</td>\n",
       "      <td>93.325963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>580.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>62957.0</td>\n",
       "      <td>-502.0</td>\n",
       "      <td>3278.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>-5041.0</td>\n",
       "      <td>9.0658</td>\n",
       "      <td>545.700404</td>\n",
       "      <td>539.847445</td>\n",
       "      <td>526.412277</td>\n",
       "      <td>13.435169</td>\n",
       "      <td>8.026473</td>\n",
       "      <td>94.939847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-11-20</td>\n",
       "      <td>576.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>26606.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>-2193.0</td>\n",
       "      <td>5.4217</td>\n",
       "      <td>570.883694</td>\n",
       "      <td>567.188910</td>\n",
       "      <td>556.797391</td>\n",
       "      <td>10.391519</td>\n",
       "      <td>7.167611</td>\n",
       "      <td>94.971748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>582.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>39881.0</td>\n",
       "      <td>-334.0</td>\n",
       "      <td>18793.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-772.0</td>\n",
       "      <td>10844.0</td>\n",
       "      <td>6.7572</td>\n",
       "      <td>573.706955</td>\n",
       "      <td>569.929078</td>\n",
       "      <td>558.886473</td>\n",
       "      <td>11.042605</td>\n",
       "      <td>7.942610</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-11-22</td>\n",
       "      <td>576.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>23922.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>-2966.0</td>\n",
       "      <td>-478.0</td>\n",
       "      <td>-230.0</td>\n",
       "      <td>-7073.0</td>\n",
       "      <td>4.7807</td>\n",
       "      <td>574.365564</td>\n",
       "      <td>571.016912</td>\n",
       "      <td>560.228216</td>\n",
       "      <td>10.788696</td>\n",
       "      <td>8.511827</td>\n",
       "      <td>97.938530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-11-23</td>\n",
       "      <td>574.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>15144.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>3740.0</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>-218.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>3.0366</td>\n",
       "      <td>575.092451</td>\n",
       "      <td>572.091233</td>\n",
       "      <td>561.544644</td>\n",
       "      <td>10.546589</td>\n",
       "      <td>8.918779</td>\n",
       "      <td>95.718908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-11-24</td>\n",
       "      <td>577.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>12503.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>-854.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-118.0</td>\n",
       "      <td>-2263.0</td>\n",
       "      <td>2.8318</td>\n",
       "      <td>575.073961</td>\n",
       "      <td>572.538736</td>\n",
       "      <td>562.541337</td>\n",
       "      <td>9.997398</td>\n",
       "      <td>9.134503</td>\n",
       "      <td>90.744592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   open   high    low  close   volume  financing       fi   \n",
       "0    2021-01-04  530.0  540.0  528.0  536.0  39490.0      454.0  12463.0  \\\n",
       "1    2021-01-05  536.0  542.0  535.0  542.0  34839.0     -355.0   2884.0   \n",
       "2    2021-01-06  555.0  555.0  541.0  549.0  55614.0     -256.0   5355.0   \n",
       "3    2021-01-07  554.0  570.0  553.0  565.0  53393.0     2200.0   1671.0   \n",
       "4    2021-01-08  580.0  580.0  571.0  580.0  62957.0     -502.0   3278.0   \n",
       "..          ...    ...    ...    ...    ...      ...        ...      ...   \n",
       "699  2023-11-20  576.0  579.0  575.0  577.0  26606.0      176.0   3579.0   \n",
       "700  2023-11-21  582.0  585.0  581.0  585.0  39881.0     -334.0  18793.0   \n",
       "701  2023-11-22  576.0  579.0  574.0  577.0  23922.0      533.0  -2966.0   \n",
       "702  2023-11-23  574.0  578.0  574.0  578.0  15144.0      173.0   3740.0   \n",
       "703  2023-11-24  577.0  578.0  574.0  575.0  12503.0      243.0   -854.0   \n",
       "\n",
       "        ii      di       rp  capital        EMA9       EMA12       EMA26   \n",
       "0    -33.0   865.0   2342.0   6.0443  521.295251  518.980386  513.251221  \\\n",
       "1    179.0  -451.0  -1374.0   5.3592  525.437881  522.532126  515.535238   \n",
       "2    105.0 -4163.0      1.0   6.9696  530.151835  526.614084  518.179719   \n",
       "3    -75.0  2060.0   -402.0   8.7664  537.123278  532.531850  521.861371   \n",
       "4    187.0  1176.0  -5041.0   9.0658  545.700404  539.847445  526.412277   \n",
       "..     ...     ...      ...      ...         ...         ...         ...   \n",
       "699 -125.0   270.0  -2193.0   5.4217  570.883694  567.188910  556.797391   \n",
       "700   97.0  -772.0  10844.0   6.7572  573.706955  569.929078  558.886473   \n",
       "701 -478.0  -230.0  -7073.0   4.7807  574.365564  571.016912  560.228216   \n",
       "702 -253.0  -218.0     93.0   3.0366  575.092451  572.091233  561.544644   \n",
       "703   70.0  -118.0  -2263.0   2.8318  575.073961  572.538736  562.541337   \n",
       "\n",
       "          MACD    Signal       RSI14  y_5  \n",
       "0     5.729165  3.933239   84.477581    1  \n",
       "1     6.996887  4.619674   88.417310    1  \n",
       "2     8.434365  5.454306   91.005801    1  \n",
       "3    10.670478  6.574521   93.325963    1  \n",
       "4    13.435169  8.026473   94.939847    1  \n",
       "..         ...       ...         ...  ...  \n",
       "699  10.391519  7.167611   94.971748    0  \n",
       "700  11.042605  7.942610  100.000000    0  \n",
       "701  10.788696  8.511827   97.938530    0  \n",
       "702  10.546589  8.918779   95.718908    0  \n",
       "703   9.997398  9.134503   90.744592    0  \n",
       "\n",
       "[704 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "TICKER = 2330\n",
    "TP = 5\n",
    "#############\n",
    "\n",
    "### import data ###\n",
    "data = pd.read_csv('/Users/yitsung/Desktop/MasterThesis/data/TaiwanStockData_Top100_EMA')\n",
    "ticker_data = data[data['ticker']==TICKER].reset_index(drop=True)\n",
    "ticker_data = ticker_data.drop(columns=['ticker'])\n",
    "\n",
    "### generate y ###\n",
    "\n",
    "# ver.1(P-P, 2class) #\n",
    "# ticker_data[f'y_after_{TP}'] = ticker_data['close'].shift(-TP)\n",
    "# ticker_data[f'y_after_{TP}'] = ticker_data[f'y_after_{TP}'] - ticker_data['close']\n",
    "# ticker_data = ticker_data.dropna().reindex()\n",
    "# ticker_data[f'y_after_{TP}'] = (ticker_data[f'y_after_{TP}'] >= 0).astype(int)\n",
    "\n",
    "# ver.2(SMA-P/P, 2class) #\n",
    "ticker_data[f'y_{TP}'] = ticker_data['close'].rolling(window=TP).mean()\n",
    "ticker_data[f'y_{TP}'] = ticker_data[f'y_{TP}'].shift(-TP)\n",
    "ticker_data = ticker_data.dropna().reindex()\n",
    "ticker_data[f'y_{TP}'] = ((ticker_data[f'y_{TP}'] - ticker_data['close']) >= 0).astype(int)\n",
    "\n",
    "# ver.3(SMA-SMA/SMA, 3class) #\n",
    "########## not yet ###########\n",
    "\n",
    "# ### origi data ###\n",
    "# origi_data = ticker_data.copy()\n",
    "\n",
    "# ### diff data ###\n",
    "# ticker_data['open'] = ticker_data['open'].diff()\n",
    "# ticker_data['high'] = ticker_data['high'].diff()\n",
    "# ticker_data['low'] = ticker_data['low'].diff()\n",
    "# ticker_data['close'] = ticker_data['close'].diff()\n",
    "\n",
    "# ticker_data['EMA9'] = ticker_data['EMA9'].diff()\n",
    "# ticker_data['EMA12'] = ticker_data['EMA12'].diff()\n",
    "# ticker_data['EMA26'] = ticker_data['EMA26'].diff()\n",
    "\n",
    "# ticker_data.replace([float('inf'), -float('inf')], 0, inplace=True) # 不知道為何有些調整過後會變inf, 要拿掉(應該是連兩天的價格都相同)\n",
    "# ticker_data = ticker_data.dropna().reset_index(drop=True)\n",
    "\n",
    "# ### move 'y' to the last column ###\n",
    "# y_column = ticker_data.pop(f'y_after_{TP}')\n",
    "# ticker_data[f'y_after_{TP}'] = y_column\n",
    "\n",
    "ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.Splite data into train(Library) and test(Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>financing</th>\n",
       "      <th>fi</th>\n",
       "      <th>ii</th>\n",
       "      <th>di</th>\n",
       "      <th>rp</th>\n",
       "      <th>capital</th>\n",
       "      <th>EMA9</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>y_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>544.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>17137.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-2573.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>-142.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>3.9095</td>\n",
       "      <td>544.649068</td>\n",
       "      <td>543.673118</td>\n",
       "      <td>541.716547</td>\n",
       "      <td>1.956572</td>\n",
       "      <td>0.734048</td>\n",
       "      <td>91.909547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2023-10-26</td>\n",
       "      <td>530.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>31683.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>-10712.0</td>\n",
       "      <td>-35173.0</td>\n",
       "      <td>-1744.0</td>\n",
       "      <td>-10261.0</td>\n",
       "      <td>6.9033</td>\n",
       "      <td>541.919255</td>\n",
       "      <td>541.723408</td>\n",
       "      <td>540.922728</td>\n",
       "      <td>0.800679</td>\n",
       "      <td>0.747374</td>\n",
       "      <td>85.178131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>2023-10-27</td>\n",
       "      <td>534.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>17051.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-5262.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-1739.0</td>\n",
       "      <td>4.1968</td>\n",
       "      <td>540.135404</td>\n",
       "      <td>540.381345</td>\n",
       "      <td>540.335860</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>78.927028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>2023-10-30</td>\n",
       "      <td>531.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>23299.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-11811.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>-5803.0</td>\n",
       "      <td>5.6532</td>\n",
       "      <td>538.508323</td>\n",
       "      <td>539.091907</td>\n",
       "      <td>539.718389</td>\n",
       "      <td>-0.626481</td>\n",
       "      <td>0.360301</td>\n",
       "      <td>72.836426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>535.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>28073.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>-9363.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>-358.0</td>\n",
       "      <td>-5392.0</td>\n",
       "      <td>5.4314</td>\n",
       "      <td>536.606658</td>\n",
       "      <td>537.539306</td>\n",
       "      <td>538.924434</td>\n",
       "      <td>-1.385128</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>63.359478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   open   high    low  close   volume  financing       fi   \n",
       "681  2023-10-25  544.0  551.0  544.0  544.0  17137.0      -99.0  -2573.0  \\\n",
       "682  2023-10-26  530.0  535.0  530.0  531.0  31683.0      487.0 -10712.0   \n",
       "683  2023-10-27  534.0  536.0  532.0  533.0  17051.0       17.0  -5262.0   \n",
       "684  2023-10-30  531.0  534.0  528.0  532.0  23299.0      265.0 -11811.0   \n",
       "685  2023-10-31  535.0  535.0  527.0  529.0  28073.0      113.0  -9363.0   \n",
       "\n",
       "          ii      di       rp  capital        EMA9       EMA12       EMA26   \n",
       "681    651.0  -142.0   1185.0   3.9095  544.649068  543.673118  541.716547  \\\n",
       "682 -35173.0 -1744.0 -10261.0   6.9033  541.919255  541.723408  540.922728   \n",
       "683   1478.0   -73.0  -1739.0   4.1968  540.135404  540.381345  540.335860   \n",
       "684    487.0   378.0  -5803.0   5.6532  538.508323  539.091907  539.718389   \n",
       "685    495.0  -358.0  -5392.0   5.4314  536.606658  537.539306  538.924434   \n",
       "\n",
       "         MACD    Signal      RSI14  y_5  \n",
       "681  1.956572  0.734048  91.909547    0  \n",
       "682  0.800679  0.747374  85.178131    1  \n",
       "683  0.045485  0.606996  78.927028    1  \n",
       "684 -0.626481  0.360301  72.836426    1  \n",
       "685 -1.385128  0.011215  63.359478    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Library = ticker_data[ticker_data['Date'] <= '2023-06-30'] # windows=20, 最後預測到6/30\n",
    "Prediction = ticker_data[(ticker_data['Date'] >= '2023-06-01')&(ticker_data['Date'] <= '2023-10-31')] # windows=20, 從6/1預測7/3開始\n",
    "Prediction.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.Data Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_minmax(Library, Prediction):\n",
    "\n",
    "    # MinMax #\n",
    "    scaler_X = MinMaxScaler()\n",
    "    feature_to_standardize = Library.columns.to_list()[1 : -1]\n",
    "    Library[feature_to_standardize] = scaler_X.fit_transform(Library[feature_to_standardize])\n",
    "    Prediction[feature_to_standardize] = scaler_X.fit_transform(Prediction[feature_to_standardize])\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    Lib_ans = Library.iloc[:, -1].values.reshape(-1, 1)\n",
    "    Library.iloc[:, -1] = scaler_y.fit_transform(Lib_ans)\n",
    "    Pred_ans = Prediction.iloc[:, -1].values.reshape(-1, 1)\n",
    "    Prediction.iloc[:, -1] = scaler_y.fit_transform(Pred_ans)\n",
    "\n",
    "    return Library, Prediction, scaler_y\n",
    "\n",
    "### 切train和validation ###\n",
    "train_Library = Library[: int((len(Library) * 0.8))]\n",
    "valid_Library = Library[int((len(Library) * 0.8)): ]\n",
    "train_Library, valid_Library, _ = make_data_minmax(Library=train_Library, Prediction=valid_Library)\n",
    "\n",
    "### 切完整data ###\n",
    "Library, Prediction, _ = make_data_minmax(Library=Library, Prediction=Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.Make window data: X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data, window_size):\n",
    "\n",
    "    X = np.array(data.iloc[:, 1: -1])\n",
    "    y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "    data_X, data_y = [], []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        data_X.append(X[i : (i + window_size), :])\n",
    "        data_y.append(y[i + window_size - 1])\n",
    "\n",
    "    data_X, data_y = np.array(data_X), np.array(data_y)\n",
    "        \n",
    "    return data_X, data_y\n",
    "\n",
    "train_X, train_y = data_preprocess(data=train_Library, window_size=20)\n",
    "valid_X, valid_y = data_preprocess(data=valid_Library, window_size=20)\n",
    "full_X, full_y = data_preprocess(data=Library, window_size=20)\n",
    "test_X, test_y = data_preprocess(data=Prediction, window_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5.Over-smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resampled train_X: (508, 20, 17)\n",
      "Shape of resampled train_y: (508, 1)\n",
      "Number of positive samples after resampling: 254\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=87) # 初始化過採樣器\n",
    "train_X_resampled, train_y_resampled = ros.fit_resample(train_X.reshape(train_X.shape[0], -1), train_y)# 對訓練集進行過採樣\n",
    "train_X_resampled = train_X_resampled.reshape(-1, train_X.shape[1], train_X.shape[2]) # 將過採樣後的數據重新整形成原來的格式\n",
    "\n",
    "# ############\n",
    "train_y_resampled = train_y_resampled.reshape(-1,1)\n",
    "# ############\n",
    "\n",
    "# 檢查過採樣後的資料大小 #\n",
    "print(\"Shape of resampled train_X:\", train_X_resampled.shape)\n",
    "print(\"Shape of resampled train_y:\", train_y_resampled.shape)\n",
    "# 檢查過採樣後的正類樣本數量 #\n",
    "print(\"Number of positive samples after resampling:\", train_y_resampled.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resampled full_X: (630, 20, 17)\n",
      "Shape of resampled full_y: (630, 1)\n",
      "Number of positive samples after resampling: 315\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=87) # 初始化過採樣器\n",
    "full_X_resampled, full_y_resampled = ros.fit_resample(full_X.reshape(full_X.shape[0], -1), full_y)# 對訓練集進行過採樣\n",
    "full_X_resampled = full_X_resampled.reshape(-1, full_X.shape[1], full_X.shape[2]) # 將過採樣後的數據重新整形成原來的格式\n",
    "\n",
    "# ############\n",
    "full_y_resampled = full_y_resampled.reshape(-1,1)\n",
    "# ############\n",
    "\n",
    "# 檢查過採樣後的資料大小 #\n",
    "print(\"Shape of resampled full_X:\", full_X_resampled.shape)\n",
    "print(\"Shape of resampled full_y:\", full_y_resampled.shape)\n",
    "# 檢查過採樣後的正類樣本數量 #\n",
    "print(\"Number of positive samples after resampling:\", full_y_resampled.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "TUNNING = False\n",
    "\n",
    "params = {'X_shape': train_X.shape,\n",
    "          'hidden_units': [96, 64, 96, 112, 48], \n",
    "          'dropout_rates': [0.5, 0.0],\n",
    "          'ls': 1e-05, 'lr': 1e-04}\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.Create model and find hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_model(hp, X_shape):\n",
    "\n",
    "    # tf.random.set_seed(87)\n",
    "\n",
    "    #############################################\n",
    "    hidden_units = [hp.Int(name=f\"units_{i}\", min_value=16, max_value=256, step=16) for i in range(1, 6)]\n",
    "    dropout_rates = [hp.Choice(f\"dropout_{i}\", [0.0, 0.2, 0.5, 0.8]) for i in range(1, 3)]\n",
    "    ls = hp.Choice('ls',[1e-2, 1e-3, 1e-5])\n",
    "    lr = hp.Choice('lr',[1e-2, 1e-3, 1e-5])\n",
    "    #############################################\n",
    "    \n",
    "    inp = Input(shape = (X_shape[1], X_shape[2]))\n",
    "\n",
    "    x = LSTM(hidden_units[0], return_sequences=True)(inp)\n",
    "    x = LSTM(hidden_units[1], return_sequences=True)(x)\n",
    "    x = Attention(hidden_units[2])(x)\n",
    "    \n",
    "    x = Dense(hidden_units[3])(x)\n",
    "    x = Dropout(dropout_rates[0])(x)\n",
    "    x = Dense(hidden_units[4])(x)\n",
    "    x = Dropout(dropout_rates[1])(x)\n",
    "\n",
    "    out = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                  metrics=tf.keras.metrics.AUC(name='AUC'))\n",
    "\n",
    "    return model\n",
    "\n",
    "if TUNNING:\n",
    "    model_fn = lambda hp: tunning_model(hp, X_shape=train_X.shape)\n",
    "    tuner = kt.BayesianOptimization(model_fn,\n",
    "                                    objective=kt.Objective(\"val_AUC\", direction=\"max\"),\n",
    "                                    max_trials=10,\n",
    "                                    executions_per_trial=2,\n",
    "                                    directory=\"model_kt\",\n",
    "                                    overwrite=True,\n",
    "                                    seed=87)\n",
    "    path = f'model.hdf5'\n",
    "    ckp = ModelCheckpoint(path, monitor='val_AUC', verbose = 0, \n",
    "                          save_best_only=True, save_weights_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_AUC', min_delta=1e-4, patience=10, mode='max', \n",
    "                       baseline=None, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    tuner.search(train_X_resampled, train_y_resampled, validation_data=(valid_X, valid_y),\n",
    "                 epochs=100, batch_size=16, callbacks=[ckp, es], verbose=1)\n",
    "    model = tuner.get_best_models()[0]\n",
    "\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.Train model(with parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:34:55.205232: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 0.6925 - AUC: 0.5246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:34:57.198458: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 53ms/step - loss: 0.6925 - AUC: 0.5246 - val_loss: 0.7234 - val_AUC: 0.6217\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6894 - AUC: 0.5228 - val_loss: 0.7142 - val_AUC: 0.6555\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6878 - AUC: 0.5443 - val_loss: 0.7154 - val_AUC: 0.6608\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6918 - AUC: 0.5171 - val_loss: 0.7052 - val_AUC: 0.6615\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6849 - AUC: 0.5543 - val_loss: 0.7066 - val_AUC: 0.6574\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6803 - AUC: 0.5920 - val_loss: 0.7232 - val_AUC: 0.6461\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6780 - AUC: 0.5953 - val_loss: 0.7142 - val_AUC: 0.6486\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6771 - AUC: 0.6049 - val_loss: 0.7126 - val_AUC: 0.6400\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6760 - AUC: 0.6065 - val_loss: 0.7236 - val_AUC: 0.6334\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6829 - AUC: 0.5762 - val_loss: 0.7107 - val_AUC: 0.6372\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6747 - AUC: 0.6023 - val_loss: 0.7209 - val_AUC: 0.6298\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6782 - AUC: 0.5970 - val_loss: 0.7164 - val_AUC: 0.6278\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6757 - AUC: 0.6142 - val_loss: 0.6988 - val_AUC: 0.6320\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6769 - AUC: 0.5975 - val_loss: 0.6942 - val_AUC: 0.6320\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6732 - AUC: 0.6059 - val_loss: 0.7180 - val_AUC: 0.6339\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6749 - AUC: 0.6141 - val_loss: 0.6770 - val_AUC: 0.6438\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6767 - AUC: 0.5973 - val_loss: 0.7188 - val_AUC: 0.6253\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6751 - AUC: 0.6062 - val_loss: 0.6880 - val_AUC: 0.6407\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6747 - AUC: 0.6050 - val_loss: 0.7049 - val_AUC: 0.6288\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6764 - AUC: 0.6055 - val_loss: 0.6983 - val_AUC: 0.6319\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6716 - AUC: 0.6181 - val_loss: 0.7204 - val_AUC: 0.6195\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6717 - AUC: 0.6156 - val_loss: 0.7132 - val_AUC: 0.6298\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6660 - AUC: 0.6334 - val_loss: 0.7025 - val_AUC: 0.6301\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6765 - AUC: 0.6021 - val_loss: 0.7215 - val_AUC: 0.6286\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6706 - AUC: 0.6195 - val_loss: 0.7045 - val_AUC: 0.6307\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6685 - AUC: 0.6307 - val_loss: 0.7174 - val_AUC: 0.6326\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.6711 - AUC: 0.6120 - val_loss: 0.6851 - val_AUC: 0.6336\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6686 - AUC: 0.6283 - val_loss: 0.6884 - val_AUC: 0.6286\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6688 - AUC: 0.6236 - val_loss: 0.6950 - val_AUC: 0.6317\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6615 - AUC: 0.6405 - val_loss: 0.6930 - val_AUC: 0.6307\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6627 - AUC: 0.6412 - val_loss: 0.7323 - val_AUC: 0.6278\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.6699 - AUC: 0.6236 - val_loss: 0.6897 - val_AUC: 0.6374\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6593 - AUC: 0.6502 - val_loss: 0.7473 - val_AUC: 0.6221\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.6635 - AUC: 0.6336 - val_loss: 0.7031 - val_AUC: 0.6363\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.6637 - AUC: 0.6401 - val_loss: 0.7052 - val_AUC: 0.6409\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6636 - AUC: 0.6360 - val_loss: 0.6727 - val_AUC: 0.6387\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6593 - AUC: 0.6507 - val_loss: 0.6851 - val_AUC: 0.6341\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6601 - AUC: 0.6467 - val_loss: 0.7126 - val_AUC: 0.6301\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6594 - AUC: 0.6395 - val_loss: 0.7369 - val_AUC: 0.6237\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6645 - AUC: 0.6316 - val_loss: 0.7349 - val_AUC: 0.6234\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6605 - AUC: 0.6462 - val_loss: 0.6875 - val_AUC: 0.6323\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6566 - AUC: 0.6510 - val_loss: 0.7160 - val_AUC: 0.6331\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6551 - AUC: 0.6521 - val_loss: 0.7005 - val_AUC: 0.6250\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6524 - AUC: 0.6602 - val_loss: 0.6848 - val_AUC: 0.6364\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6542 - AUC: 0.6582 - val_loss: 0.7242 - val_AUC: 0.6258\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6539 - AUC: 0.6612 - val_loss: 0.6927 - val_AUC: 0.6267\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.6527 - AUC: 0.6555 - val_loss: 0.6957 - val_AUC: 0.6281\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6524 - AUC: 0.6550 - val_loss: 0.6820 - val_AUC: 0.6320\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6561 - AUC: 0.6497 - val_loss: 0.7427 - val_AUC: 0.6227\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6473 - AUC: 0.6679 - val_loss: 0.6992 - val_AUC: 0.6250\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6515 - AUC: 0.6522 - val_loss: 0.6640 - val_AUC: 0.6415\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6457 - AUC: 0.6723 - val_loss: 0.6934 - val_AUC: 0.6284\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6484 - AUC: 0.6659 - val_loss: 0.6578 - val_AUC: 0.6397\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6448 - AUC: 0.6699 - val_loss: 0.6830 - val_AUC: 0.6297\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6450 - AUC: 0.6669 - val_loss: 0.7005 - val_AUC: 0.6372\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6459 - AUC: 0.6746 - val_loss: 0.6852 - val_AUC: 0.6383\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6691 - AUC: 0.6338 - val_loss: 0.6945 - val_AUC: 0.6195\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6542 - AUC: 0.6546 - val_loss: 0.7033 - val_AUC: 0.6353\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6429 - AUC: 0.6781 - val_loss: 0.7377 - val_AUC: 0.6264\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6412 - AUC: 0.6783 - val_loss: 0.6956 - val_AUC: 0.6321\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6471 - AUC: 0.6716 - val_loss: 0.6775 - val_AUC: 0.6452\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6468 - AUC: 0.6667 - val_loss: 0.6455 - val_AUC: 0.6405\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6560 - AUC: 0.6487 - val_loss: 0.6739 - val_AUC: 0.6364\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6424 - AUC: 0.6701 - val_loss: 0.6746 - val_AUC: 0.6410\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6407 - AUC: 0.6753 - val_loss: 0.7019 - val_AUC: 0.6340\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6380 - AUC: 0.6830 - val_loss: 0.6848 - val_AUC: 0.6413\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6338 - AUC: 0.6887 - val_loss: 0.7301 - val_AUC: 0.6321\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6321 - AUC: 0.6936 - val_loss: 0.6718 - val_AUC: 0.6429\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6492 - AUC: 0.6678 - val_loss: 0.6671 - val_AUC: 0.6410\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6374 - AUC: 0.6860 - val_loss: 0.7170 - val_AUC: 0.6399\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6355 - AUC: 0.6890 - val_loss: 0.7026 - val_AUC: 0.6397\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6341 - AUC: 0.6893 - val_loss: 0.6742 - val_AUC: 0.6311\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6397 - AUC: 0.6768 - val_loss: 0.7403 - val_AUC: 0.6356\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6386 - AUC: 0.6780 - val_loss: 0.6786 - val_AUC: 0.6341\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6336 - AUC: 0.6887 - val_loss: 0.6877 - val_AUC: 0.6370\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6342 - AUC: 0.6881 - val_loss: 0.6921 - val_AUC: 0.6438\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6336 - AUC: 0.6907 - val_loss: 0.6442 - val_AUC: 0.6439\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6365 - AUC: 0.6840 - val_loss: 0.6973 - val_AUC: 0.6409\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6254 - AUC: 0.6992 - val_loss: 0.6772 - val_AUC: 0.6400\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6243 - AUC: 0.7023 - val_loss: 0.7463 - val_AUC: 0.6366\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6265 - AUC: 0.6981 - val_loss: 0.7106 - val_AUC: 0.6415\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6260 - AUC: 0.7043 - val_loss: 0.6696 - val_AUC: 0.6436\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6211 - AUC: 0.7110 - val_loss: 0.6426 - val_AUC: 0.6382\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6281 - AUC: 0.6958 - val_loss: 0.6561 - val_AUC: 0.6353\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6233 - AUC: 0.7039 - val_loss: 0.7158 - val_AUC: 0.6379\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6225 - AUC: 0.7029 - val_loss: 0.7161 - val_AUC: 0.6405\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6268 - AUC: 0.6908 - val_loss: 0.7099 - val_AUC: 0.6304\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6162 - AUC: 0.7198 - val_loss: 0.7040 - val_AUC: 0.6313\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6078 - AUC: 0.7228 - val_loss: 0.7319 - val_AUC: 0.6430\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6144 - AUC: 0.7112 - val_loss: 0.6834 - val_AUC: 0.6395\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6133 - AUC: 0.7164 - val_loss: 0.6626 - val_AUC: 0.6270\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6126 - AUC: 0.7152 - val_loss: 0.7269 - val_AUC: 0.6291\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6153 - AUC: 0.7070 - val_loss: 0.6720 - val_AUC: 0.6220\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6082 - AUC: 0.7266 - val_loss: 0.6619 - val_AUC: 0.6396\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6130 - AUC: 0.7219 - val_loss: 0.6936 - val_AUC: 0.6271\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6148 - AUC: 0.7077 - val_loss: 0.6983 - val_AUC: 0.6242\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6138 - AUC: 0.7132 - val_loss: 0.7296 - val_AUC: 0.6263\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6129 - AUC: 0.7154 - val_loss: 0.6801 - val_AUC: 0.6428\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6021 - AUC: 0.7278 - val_loss: 0.6553 - val_AUC: 0.6337\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.6108 - AUC: 0.7097 - val_loss: 0.6852 - val_AUC: 0.6344\n",
      "AUC: 0.66154944896698\n"
     ]
    }
   ],
   "source": [
    "def create_model(X_shape, hidden_units, dropout_rates, lr, ls):\n",
    "\n",
    "    tf.random.set_seed(87)\n",
    "\n",
    "    inp = Input(shape = (X_shape[1], X_shape[2]))\n",
    "\n",
    "    x = LSTM(hidden_units[0], return_sequences=True)(inp)\n",
    "    x = LSTM(hidden_units[1], return_sequences=True)(x)\n",
    "    x = Attention(hidden_units[2])(x)\n",
    "    \n",
    "    x = Dense(hidden_units[3])(x)\n",
    "    x = Dropout(dropout_rates[0])(x)\n",
    "    x = Dense(hidden_units[4])(x)\n",
    "    x = Dropout(dropout_rates[1])(x)\n",
    "\n",
    "    out = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr), # or RMSprop\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=ls), \n",
    "                  metrics=tf.keras.metrics.AUC(name='AUC'))\n",
    "\n",
    "    return model\n",
    "\n",
    "if TUNNING == False:\n",
    "\n",
    "    path = f'model.hdf5'\n",
    "    model = create_model(**params)\n",
    "    # ckp = ModelCheckpoint(path, monitor='val_AUC', verbose = 0, \n",
    "    #                       save_best_only=True, save_weights_only=True, mode='max')\n",
    "    # es = EarlyStopping(monitor='val_AUC', min_delta=1e-4, patience=10, mode='max', # 若不EarlyStopping就先拿掉\n",
    "    #                    baseline=None, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    history = model.fit(full_X_resampled, full_y_resampled, # train_X_resampled, train_y_resampled,\n",
    "                        validation_split=0.2, shuffle=True, # validation_data=(valid_X, valid_y),                   \n",
    "                        # sample_weight = sw[tr], \n",
    "                        epochs=100, batch_size=16,\n",
    "                        # callbacks=[ckp, es], # 不EarlyStopping所以先拿掉\n",
    "                        verbose=1) \n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    score = hist['val_AUC'].max()\n",
    "    print(f'AUC:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.Test model on one stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:36:09.468672: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 44ms/step\n",
      "ACC: 0.5238095238095238\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pred  True\n",
       "0      1     0\n",
       "1      1     0\n",
       "2      1     0\n",
       "3      1     0\n",
       "4      1     1\n",
       "5      1     1\n",
       "6      1     1\n",
       "7      1     1\n",
       "8      1     1\n",
       "9      0     0\n",
       "10     0     0\n",
       "11     0     0\n",
       "12     0     0\n",
       "13     0     0\n",
       "14     0     0\n",
       "15     0     1\n",
       "16     1     1\n",
       "17     1     0\n",
       "18     1     0\n",
       "19     1     0\n",
       "20     1     0\n",
       "21     1     0\n",
       "22     1     0\n",
       "23     1     0\n",
       "24     1     0\n",
       "25     1     0\n",
       "26     1     0\n",
       "27     1     0\n",
       "28     1     0\n",
       "29     1     0\n",
       "30     1     0\n",
       "31     1     0\n",
       "32     1     1\n",
       "33     1     1\n",
       "34     1     1\n",
       "35     1     1\n",
       "36     1     1\n",
       "37     1     1\n",
       "38     1     0\n",
       "39     1     1\n",
       "40     1     1\n",
       "41     1     1\n",
       "42     1     0\n",
       "43     1     1\n",
       "44     1     1\n",
       "45     1     0\n",
       "46     1     0\n",
       "47     1     0\n",
       "48     1     1\n",
       "49     1     1\n",
       "50     1     1\n",
       "51     1     1\n",
       "52     1     1\n",
       "53     1     0\n",
       "54     1     0\n",
       "55     1     0\n",
       "56     1     0\n",
       "57     1     0\n",
       "58     1     0\n",
       "59     1     1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = model.predict(test_X)\n",
    "pred_dir = (pred_dir > 0.5).astype(int)\n",
    "\n",
    "result_df = pd.DataFrame(pred_dir, columns=['Pred'])\n",
    "result_df['True'] = test_y\n",
    "\n",
    "match_count = (result_df['Pred'] == result_df['True']).sum()\n",
    "correct = match_count / len(result_df)\n",
    "\n",
    "print(f'ACC: {correct}\\n')\n",
    "result_df.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
